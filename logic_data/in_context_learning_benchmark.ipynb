{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6127bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d20e1",
   "metadata": {},
   "source": [
    "### Benchmark Creation\n",
    "- logic operators: ==, !=, or, and\n",
    "- arity of 3: a, b, c unique boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1b1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "set_seed(seed)\n",
    "SKIP = True\n",
    "if not SKIP:\n",
    "    for i in range(len(L)):\n",
    "        L[i] = L[i].replace('\\\"', '')\n",
    "        for key in table:\n",
    "            L[i] = L[i].replace(key, table[key])\n",
    "\n",
    "    cfg = CFG(*L)\n",
    "    primitive_clauses = set([])\n",
    "    equivalent_clauses = set([])\n",
    "    for _ in range(10000): # 10000 will get you all the programs.\n",
    "        clause = generateSentence(cfg)\n",
    "        if \"a == a\" in clause or \\\n",
    "            \"a != a\" in clause or \\\n",
    "            \"b == b\" in clause or \\\n",
    "            \"b != b\" in clause or \\\n",
    "            \"c == c\" in clause or \\\n",
    "            \"c != c\" in clause:\n",
    "            continue\n",
    "\n",
    "        if \"and\" in clause and \\\n",
    "            clause.split(\" and \")[0] == clause.split(\" and \")[1]:\n",
    "            continue\n",
    "        if \"or\" in clause and \\\n",
    "            clause.split(\" or \")[0] == clause.split(\" or \")[1]:\n",
    "            continue\n",
    "\n",
    "        if \"and\" in clause:\n",
    "            variables = set([])\n",
    "            for c in clause.split(\" and \"):\n",
    "                if \"==\" in c:\n",
    "                    for t in c.strip(\"() \").split(\" == \"):\n",
    "                        variables.add(t)\n",
    "                elif \"!=\" in c:\n",
    "                    for t in c.strip(\"() \").split(\" != \"):\n",
    "                        variables.add(t)\n",
    "            if len(variables) < 3:\n",
    "                continue\n",
    "\n",
    "            left = sorted(clause.split(\" and \")[0].split())\n",
    "            right = sorted(clause.split(\" and \")[1].split())\n",
    "\n",
    "            if (\"and\", tuple(left), tuple(right)) in equivalent_clauses or \\\n",
    "                (\"and\", tuple(right), tuple(left)) in equivalent_clauses:\n",
    "                continue\n",
    "            else:\n",
    "                equivalent_clauses.add((\"and\", tuple(left), tuple(right)))\n",
    "                equivalent_clauses.add((\"and\", tuple(right), tuple(left)))\n",
    "\n",
    "        if \"or\" in clause:\n",
    "            variables = set([])\n",
    "            for c in clause.split(\" or \"):\n",
    "                if \"==\" in c:\n",
    "                    for t in c.strip(\"() \").split(\" == \"):\n",
    "                        variables.add(t)\n",
    "                elif \"!=\" in c:\n",
    "                    for t in c.strip(\"() \").split(\" != \"):\n",
    "                        variables.add(t)\n",
    "            if len(variables) < 3:\n",
    "                continue\n",
    "\n",
    "            left = sorted(clause.split(\" or \")[0].split())\n",
    "            right = sorted(clause.split(\" or \")[1].split())\n",
    "            \n",
    "            if (\"or\", tuple(left), tuple(right)) in equivalent_clauses or \\\n",
    "                (\"or\", tuple(right), tuple(left)) in equivalent_clauses:\n",
    "                continue\n",
    "            else:\n",
    "                equivalent_clauses.add((\"or\", tuple(left), tuple(right)))\n",
    "                equivalent_clauses.add((\"or\", tuple(right), tuple(left)))\n",
    "\n",
    "        primitive_clauses.add(clause)\n",
    "\n",
    "\n",
    "    primitive_clauses = list(primitive_clauses)\n",
    "    random.shuffle(primitive_clauses)\n",
    "    training_clauses = primitive_clauses[:20]\n",
    "    eval_clauses = primitive_clauses[20:]\n",
    "    pickle.dump(primitive_clauses, open(\"./cfg_all.pkl\", 'wb'))\n",
    "    pickle.dump(training_clauses, open(\"./cfg_train.pkl\", 'wb'))\n",
    "    pickle.dump(eval_clauses, open(\"./cfg_test.pkl\", 'wb'))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    primitive_clauses = [\n",
    "     '( a != c ) and ( a == b )',\n",
    "     '( c != a ) or ( b != a )',\n",
    "     '( b != a ) and ( c != a )',\n",
    "     '( b == a ) or ( c != a )',\n",
    "     '( a == b ) or ( b == c )',\n",
    "     '( c == a ) and ( a == b )',\n",
    "     '( b != a ) and ( b != c )',\n",
    "     '( b == a ) or ( a == c )',\n",
    "     '( c == a ) and ( c != b )',\n",
    "     '( c == b ) and ( c != a )',\n",
    "     '( c != b ) or ( a == c )',\n",
    "     '( b != a ) or ( c == a )',\n",
    "     '( c != b ) and ( c != a )',\n",
    "     '( a == c ) or ( b == c )',\n",
    "    ]\n",
    "    random.shuffle(primitive_clauses)\n",
    "    training_clauses = primitive_clauses[:11]\n",
    "    eval_clauses = primitive_clauses[11:]\n",
    "    pickle.dump(primitive_clauses, open(\"./cfg_all.pkl\", 'wb'))\n",
    "    pickle.dump(training_clauses, open(\"./cfg_train.pkl\", 'wb'))\n",
    "    pickle.dump(eval_clauses, open(\"./cfg_test.pkl\", 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a865e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:10<00:00, 1895.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1952.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1978.39it/s]\n"
     ]
    }
   ],
   "source": [
    "n_training_examples = 20000\n",
    "n_eval_examples = 1000\n",
    "n_test_examples = 1000\n",
    "n_training_program = 7\n",
    "n_fewshot = 14\n",
    "\n",
    "#################\n",
    "#\n",
    "# DO NOT CHANGE\n",
    "#\n",
    "#################\n",
    "n_examples = n_fewshot + 1\n",
    "training_clauses = pickle.load(open(\"./cfg_train.pkl\", 'rb'))\n",
    "eval_clauses = pickle.load(open(\"./cfg_test.pkl\", 'rb'))\n",
    "if n_training_program is not None:\n",
    "    training_clauses = random.sample(training_clauses, k=n_training_program)\n",
    "    \n",
    "all_train_input_ids = []\n",
    "all_train_output_ids = []\n",
    "all_train_clauses = []\n",
    "m = {}\n",
    "for c in training_clauses:\n",
    "    m[c] = [0, 0]\n",
    "for i in tqdm(range(n_training_examples)):\n",
    "    clauses = random.choice(training_clauses)\n",
    "    demostrations = sample_demonstrations_for_clauses(\n",
    "        clauses,\n",
    "        final_values=[random.choice([True, False]) for i in range(n_examples)]\n",
    "    )\n",
    "    \n",
    "    # print(f\"c: {clauses}; a:{demostrations[0]['a']}, b:{demostrations[0]['b']}, c:{demostrations[0]['c']}, o:{demostrations[0]['output']}\")\n",
    "    \n",
    "    # listify\n",
    "    input_ids = [BOS_TOKEN_ID]\n",
    "    output_ids = [BOS_TOKEN_ID]\n",
    "    for d in demostrations:\n",
    "        m[clauses][int(d['output'])] += 1\n",
    "        output = FALSE_TOKEN_ID if d['output'] == False else TRUE_TOKEN_ID\n",
    "        input_ids += [INPUT_PREFIX_TOKEN_ID, d['a'], d['b'], d['c'], OUTPUT_PREFIX_TOKEN_ID, output, SEPARATOR_TOKEN_ID]\n",
    "        output_ids += [-100, -100, -100, -100, -100, output, -100]\n",
    "        assert len(input_ids) == len(output_ids)\n",
    "    input_ids += [EOS_TOKEN_ID]\n",
    "    output_ids += [EOS_TOKEN_ID]\n",
    "    all_train_input_ids += [input_ids]\n",
    "    all_train_output_ids += [output_ids]\n",
    "    all_train_clauses += [clauses]\n",
    "    \n",
    "all_eval_input_ids = []\n",
    "all_eval_output_ids = []\n",
    "all_eval_clauses = []\n",
    "for i in tqdm(range(n_eval_examples)):\n",
    "    clauses = random.choice(training_clauses)\n",
    "    demostrations = sample_demonstrations_for_clauses(\n",
    "        clauses,\n",
    "        final_values=[random.choice([True, False]) for i in range(n_examples)]\n",
    "    )\n",
    "    \n",
    "    # listify\n",
    "    input_ids = [BOS_TOKEN_ID]\n",
    "    output_ids = [BOS_TOKEN_ID]\n",
    "    for d in demostrations:\n",
    "        output = FALSE_TOKEN_ID if d['output'] == False else TRUE_TOKEN_ID\n",
    "        input_ids += [INPUT_PREFIX_TOKEN_ID, d['a'], d['b'], d['c'], OUTPUT_PREFIX_TOKEN_ID, output, SEPARATOR_TOKEN_ID]\n",
    "        output_ids += [-100, -100, -100, -100, -100, output, -100]\n",
    "        assert len(input_ids) == len(output_ids)\n",
    "    input_ids += [EOS_TOKEN_ID]\n",
    "    output_ids += [EOS_TOKEN_ID]\n",
    "    all_eval_input_ids += [input_ids]\n",
    "    all_eval_output_ids += [output_ids]\n",
    "    all_eval_clauses += [clauses]\n",
    "    \n",
    "train_data = {\n",
    "    \"input_ids\" : all_train_input_ids,\n",
    "    \"output_ids\" : all_train_output_ids,\n",
    "    \"clauses\" : all_train_clauses,\n",
    "}\n",
    "dev_data = {\n",
    "    \"input_ids\" : all_eval_input_ids,\n",
    "    \"output_ids\" : all_eval_output_ids,\n",
    "    \"clauses\" : all_eval_clauses,\n",
    "}\n",
    "pickle.dump(train_data, open(f\"./train_data.n_rule.{n_training_program}.n_shot.{n_fewshot}.pkl\", 'wb'))\n",
    "pickle.dump(dev_data, open(f\"./dev_data.n_rule.{n_training_program}.n_shot.{n_fewshot}.pkl\", 'wb'))\n",
    "\n",
    "all_test_input_ids = []\n",
    "all_test_output_ids = []\n",
    "all_test_clauses = []\n",
    "for i in tqdm(range(n_test_examples)):\n",
    "    clauses = random.choice(eval_clauses)\n",
    "    demostrations = sample_demonstrations_for_clauses(\n",
    "        clauses,\n",
    "        final_values=[random.choice([True, False]) for i in range(n_examples)]\n",
    "    )\n",
    "    \n",
    "    # listify\n",
    "    input_ids = [BOS_TOKEN_ID]\n",
    "    output_ids = [BOS_TOKEN_ID]\n",
    "    for d in demostrations:\n",
    "        output = FALSE_TOKEN_ID if d['output'] == False else TRUE_TOKEN_ID\n",
    "        input_ids += [INPUT_PREFIX_TOKEN_ID, d['a'], d['b'], d['c'], OUTPUT_PREFIX_TOKEN_ID, output, SEPARATOR_TOKEN_ID]\n",
    "        output_ids += [-100, -100, -100, -100, -100, output, -100]\n",
    "        assert len(input_ids) == len(output_ids)\n",
    "    input_ids += [EOS_TOKEN_ID]\n",
    "    output_ids += [EOS_TOKEN_ID]\n",
    "    all_test_input_ids += [input_ids]\n",
    "    all_test_output_ids += [output_ids]\n",
    "    all_test_clauses += [clauses]\n",
    "    \n",
    "test_data = {\n",
    "    \"input_ids\" : all_test_input_ids,\n",
    "    \"output_ids\" : all_test_output_ids,\n",
    "    \"clauses\" : all_test_clauses,\n",
    "}\n",
    "pickle.dump(test_data, open(f\"./test_data.n_rule.{n_training_program}.n_shot.{n_fewshot}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60076cd",
   "metadata": {},
   "source": [
    "## Bayesian Rule Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shots = 8\n",
    "n_experiment = 1000\n",
    "fig, axs = plt.subplots(2, 7, figsize=(20, 5))\n",
    "\n",
    "rule_idx = 0\n",
    "for hidden_rule in training_clauses:\n",
    "\n",
    "    experiment_results = []\n",
    "    for _ in range(n_experiment):\n",
    "        prior_hit_count = [0 for _ in training_clauses]\n",
    "        for i in range(n_shots):\n",
    "            data = sample_demonstration_for_clauses(\n",
    "                hidden_rule,\n",
    "            )\n",
    "            idx = 0\n",
    "            for clauses in training_clauses:\n",
    "                eval_output = _eval(clauses, data)\n",
    "                if eval_output == data['output']:\n",
    "                    prior_hit_count[idx] += 1\n",
    "                idx += 1\n",
    "        prior_hit_count = np.array(prior_hit_count)\n",
    "        prior_hit_count = np.exp(prior_hit_count) / np.sum(np.exp(prior_hit_count))\n",
    "        experiment_results += [prior_hit_count]\n",
    "\n",
    "    # Convert the 2D list to a NumPy array\n",
    "    data = np.array(experiment_results)\n",
    "    # Calculate the mean and standard deviation of each column\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    # Set up the plot\n",
    "    \n",
    "    # Plot the mean and standard deviation as error bars\n",
    "    axs[rule_idx//7, rule_idx%7].errorbar(range(data.shape[1]), mean, yerr=std, fmt='o', capsize=5)\n",
    "    # Set the x-axis labels\n",
    "    axs[rule_idx//7, rule_idx%7].set_xticks(range(data.shape[1]))\n",
    "    axs[rule_idx//7, rule_idx%7].set_xticklabels([training_clauses[i] for i in range(data.shape[1])], rotation=90)\n",
    "    # Set the y-axis label\n",
    "    axs[rule_idx//7, rule_idx%7].set_ylabel('Value')\n",
    "    # Set the title\n",
    "    axs[rule_idx//7, rule_idx%7].set_title(f'{hidden_rule}')\n",
    "    \n",
    "    rule_idx += 1\n",
    "    \n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19591804",
   "metadata": {},
   "source": [
    "## L1\n",
    "for each rule, we want to have at least one good model and one bad model, and we need to compare accuracies between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629d4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:00<00:00, 20576.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:02<00:00, 7696.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 22175.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 7937.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# clauses = \"( a != b ) or ( c == a )\"\n",
    "clauses = \"( a != b ) and ( c == b )\"\n",
    "n_training_examples = 20000\n",
    "shared_train = True\n",
    "n_examples = 7\n",
    "\n",
    "left_aligment_data = left_aligment_sampler(\n",
    "    clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "    clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "\n",
    "left_aligment_train_data = {\n",
    "    \"base_input_ids\" : left_aligment_data[0],\n",
    "    \"base_output_ids\" : left_aligment_data[1],\n",
    "    \"source_input_ids\" : left_aligment_data[3],\n",
    "    \"source_output_ids\" : left_aligment_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_data[6],\n",
    "    \"clauses\" : left_aligment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.l1.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_train_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_data[6],\n",
    "    \"clauses\" : left_identity_alignment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.l1.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "n_testing_examples = 1000\n",
    "left_aligment_sampled_data = left_aligment_sampler(\n",
    "    clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "    clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "\n",
    "left_aligment_test_data = {\n",
    "    \"base_input_ids\" : left_aligment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_aligment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_aligment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_aligment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_sampled_data[6],\n",
    "    \"clauses\" : left_aligment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l1.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_test_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6],\n",
    "    \"clauses\" : left_identity_alignment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l1.clauses.{clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c8e19",
   "metadata": {},
   "source": [
    "## L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d96651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4778.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3361.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 5144.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3604.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# clauses = \"( a != b ) or ( c == a )\"\n",
    "clauses = \"( a != b ) and ( c == b )\"\n",
    "n_training_examples = 20000\n",
    "shared_train = False\n",
    "\n",
    "left_aligment_data = left_aligment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train\n",
    ")\n",
    "left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train\n",
    ")\n",
    "\n",
    "left_aligment_train_data = {\n",
    "    \"base_input_ids\" : left_aligment_data[0],\n",
    "    \"base_output_ids\" : left_aligment_data[1],\n",
    "    \"source_input_ids\" : left_aligment_data[3],\n",
    "    \"source_output_ids\" : left_aligment_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_data[6],\n",
    "    \"clauses\" : left_aligment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.l2.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_train_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_data[6],\n",
    "    \"clauses\" : left_identity_alignment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.l2.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "n_testing_examples = 1000\n",
    "left_aligment_sampled_data = left_aligment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train\n",
    ")\n",
    "left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train\n",
    ")\n",
    "\n",
    "left_aligment_test_data = {\n",
    "    \"base_input_ids\" : left_aligment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_aligment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_aligment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_aligment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_sampled_data[6],\n",
    "    \"clauses\" : left_aligment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l2.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_test_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6],\n",
    "    \"clauses\" : left_identity_alignment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l2.clauses.{clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a71ff",
   "metadata": {},
   "source": [
    "## L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35327377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4593.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3412.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4695.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1939.16it/s]\n"
     ]
    }
   ],
   "source": [
    "clauses = \"( a != b ) and ( c == b )\"\n",
    "source_clauses = \"( a != b ) or ( c == a )\"\n",
    "n_training_examples = 20000\n",
    "shared_train = False\n",
    "\n",
    "left_aligment_data = left_aligment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_train_data = {\n",
    "    \"base_input_ids\" : left_aligment_data[0],\n",
    "    \"base_output_ids\" : left_aligment_data[1],\n",
    "    \"source_input_ids\" : left_aligment_data[3],\n",
    "    \"source_output_ids\" : left_aligment_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_data[6],\n",
    "    \"clauses\" : left_aligment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "source_clauses_no_space = source_clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_train_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_data[6],\n",
    "    \"clauses\" : left_identity_alignment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "n_testing_examples = 1000\n",
    "left_aligment_sampled_data = left_aligment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_test_data = {\n",
    "    \"base_input_ids\" : left_aligment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_aligment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_aligment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_aligment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_sampled_data[6],\n",
    "    \"clauses\" : left_aligment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_test_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6],\n",
    "    \"clauses\" : left_identity_alignment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bb701",
   "metadata": {},
   "source": [
    "## L3+ (L3 Exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f22302b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4536.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3459.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4468.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3562.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4516.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3481.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:04<00:00, 4429.39it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:05<00:00, 3569.39it/s]\n"
     ]
    }
   ],
   "source": [
    "clauses = \"( a != b ) and ( c == b )\"\n",
    "source_clauses = \"( a != b ) or ( c == a )\"\n",
    "n_training_examples = 20000\n",
    "shared_train = False\n",
    "\n",
    "left_aligment_data = left_aligment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_data_exchange = left_aligment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "left_identity_alignment_data_exchange = left_identity_alignment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "\n",
    "left_aligment_train_data = {\n",
    "    \"base_input_ids\" : left_aligment_data[0]+left_aligment_data_exchange[0],\n",
    "    \"base_output_ids\" : left_aligment_data[1]+left_aligment_data_exchange[1],\n",
    "    \"source_input_ids\" : left_aligment_data[3]+left_aligment_data_exchange[3],\n",
    "    \"source_output_ids\" : left_aligment_data[4]+left_aligment_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_data[6]+left_aligment_data_exchange[6],\n",
    "    \"clauses\" : left_aligment_data[2]+left_aligment_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]+left_aligment_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "source_clauses_no_space = source_clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_train_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_data[0]+left_identity_alignment_data_exchange[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_data[1]+left_identity_alignment_data_exchange[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_data[3]+left_identity_alignment_data_exchange[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_data[4]+left_identity_alignment_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_data[6]+left_identity_alignment_data_exchange[6],\n",
    "    \"clauses\" : left_identity_alignment_data[2]+left_identity_alignment_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]+left_identity_alignment_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "n_testing_examples = 1000\n",
    "left_aligment_sampled_data = left_aligment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_sampled_data_exchange = left_aligment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "left_identity_alignment_sampled_data_exchange = left_identity_alignment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "\n",
    "left_aligment_test_data = {\n",
    "    \"base_input_ids\" : left_aligment_sampled_data[0]+left_aligment_sampled_data_exchange[0],\n",
    "    \"base_output_ids\" : left_aligment_sampled_data[1]+left_aligment_sampled_data_exchange[1],\n",
    "    \"source_input_ids\" : left_aligment_sampled_data[3]+left_aligment_sampled_data_exchange[3],\n",
    "    \"source_output_ids\" : left_aligment_sampled_data[4]+left_aligment_sampled_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_sampled_data[6]+left_aligment_sampled_data_exchange[6],\n",
    "    \"clauses\" : left_aligment_sampled_data[2]+left_aligment_sampled_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]+left_aligment_sampled_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_test_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_sampled_data[0]+left_identity_alignment_sampled_data_exchange[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_sampled_data[1]+left_identity_alignment_sampled_data_exchange[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_sampled_data[3]+left_identity_alignment_sampled_data_exchange[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_sampled_data[4]+left_identity_alignment_sampled_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6]+left_identity_alignment_sampled_data_exchange[6],\n",
    "    \"clauses\" : left_identity_alignment_sampled_data[2]+left_identity_alignment_sampled_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]+left_identity_alignment_sampled_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387e505",
   "metadata": {},
   "source": [
    "## L1 Unrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a83496b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:01<00:00, 17263.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20000/20000 [00:02<00:00, 9527.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 27355.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 9545.87it/s]\n"
     ]
    }
   ],
   "source": [
    "clauses = \"( a != b ) or ( c == a )\"\n",
    "# clauses = \"( a != b ) and ( c == b )\"\n",
    "n_training_examples = 20000\n",
    "shared_train = True\n",
    "n_examples = 2\n",
    "\n",
    "left_aligment_data = left_aligment_sampler(\n",
    "    clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "    clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "\n",
    "left_aligment_train_data = {\n",
    "    \"base_input_ids\" : left_aligment_data[0],\n",
    "    \"base_output_ids\" : left_aligment_data[1],\n",
    "    \"source_input_ids\" : left_aligment_data[3],\n",
    "    \"source_output_ids\" : left_aligment_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_data[6],\n",
    "    \"clauses\" : left_aligment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.l1.unrolling.{n_examples-1}.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_train_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_data[6],\n",
    "    \"clauses\" : left_identity_alignment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.l1.unrolling.{n_examples-1}.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "n_testing_examples = 1000\n",
    "left_aligment_sampled_data = left_aligment_sampler(\n",
    "    clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "    clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    ")\n",
    "\n",
    "left_aligment_test_data = {\n",
    "    \"base_input_ids\" : left_aligment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_aligment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_aligment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_aligment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_sampled_data[6],\n",
    "    \"clauses\" : left_aligment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l1.unrolling.{n_examples-1}.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_test_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6],\n",
    "    \"clauses\" : left_identity_alignment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l1.unrolling.{n_examples-1}.clauses.{clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf4851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
