{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "930a8c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, LlamaTokenizer\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re, torch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from models.modelings_alignable_llama import *\n",
    "from utils.train_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65115cf4",
   "metadata": {},
   "source": [
    "#### GPT-J Alpaca 6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693f4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"nlpcloud/instruct-gpt-j-fp16\",\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "_ = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpcloud/instruct-gpt-j-fp16\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Reason True or False for the following statement.\\n3 is equal to 1.\\n\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(\"cuda\")\n",
    "model.eval()\n",
    "outputs = model(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "pred_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "generated_tokens = tokenizer.decode(pred_labels[0])\n",
    "generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6639a5",
   "metadata": {},
   "source": [
    "#### StableLM 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf46e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"../../stablelm_7b/\",\n",
    "#     cache_dir=CACHE_DIR,\n",
    "#     torch_dtype=torch.bfloat16\n",
    "# )\n",
    "# _ = model.to(\"cuda\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"../../stablelm_7b/\",\n",
    "#     cache_dir=CACHE_DIR\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171a9cd",
   "metadata": {},
   "source": [
    "#### Loading Dolly-12B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"databricks/dolly-v2-12b\",\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "_ = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"databricks/dolly-v2-12b\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a726b",
   "metadata": {},
   "source": [
    "#### Loading Alpaca-13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99d38f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../../alpaca_13b/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"chavinlo/alpaca-13b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 13824,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "loading weights file ../../alpaca_13b/pytorch_model.bin.index.json\n",
      "Instantiating AlignableLlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a311c99b5549e9a192c6d484983c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing AlignableLlamaForCausalLM.\n",
      "\n",
      "Some weights of AlignableLlamaForCausalLM were not initialized from the model checkpoint at ../../alpaca_13b/ and are newly initialized: ['model.intervention_boundaries', 'model.intervention_population', 'model.inverse_rotate_layer.lin_layer.parametrizations.weight.original', 'model.rotate_layer.parametrizations.weight.original', 'model.inverse_rotate_layer.lin_layer.parametrizations.weight.0.base', 'model.temperature', 'model.rotate_layer.parametrizations.weight.0.base']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file ../../alpaca_13b/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at ../.cache/models--chavinlo--alpaca-13b/snapshots/464a0bd1ec16f3a7d5295a0035aff87f307e62f1/tokenizer.model\n",
      "loading file added_tokens.json from cache at ../.cache/models--chavinlo--alpaca-13b/snapshots/464a0bd1ec16f3a7d5295a0035aff87f307e62f1/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at ../.cache/models--chavinlo--alpaca-13b/snapshots/464a0bd1ec16f3a7d5295a0035aff87f307e62f1/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at ../.cache/models--chavinlo--alpaca-13b/snapshots/464a0bd1ec16f3a7d5295a0035aff87f307e62f1/tokenizer_config.json\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "alignment_config = {\n",
    "    'layer': 25,\n",
    "    \"token_range\" : [0, 1]\n",
    "}\n",
    "model = AlignableLlamaForCausalLM.from_pretrained(\n",
    "    \"../../alpaca_13b/\",\n",
    "    alignment_config=alignment_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "_ = model.to(\"cuda\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"chavinlo/alpaca-13b\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa457bb",
   "metadata": {},
   "source": [
    "#### Loading Alpaca-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6ba345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../../alpaca_7b_bf16/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"../../alpaca_7b/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.28.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32001\n",
      "}\n",
      "\n",
      "loading weights file ../../alpaca_7b_bf16/pytorch_model.bin.index.json\n",
      "Instantiating AlignableLlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bbd0ef32bb47afa5ce45f57bfc4d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing AlignableLlamaForCausalLM.\n",
      "\n",
      "Some weights of AlignableLlamaForCausalLM were not initialized from the model checkpoint at ../../alpaca_7b_bf16/ and are newly initialized: ['model.inverse_rotate_layer.lin_layer.parametrizations.weight.0.base', 'model.temperature', 'model.rotate_layer.parametrizations.weight.original', 'model.inverse_rotate_layer.lin_layer.parametrizations.weight.original', 'model.rotate_layer.parametrizations.weight.0.base', 'model.intervention_population', 'model.intervention_boundaries']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file ../../alpaca_7b_bf16/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": -1,\n",
      "  \"transformers_version\": \"4.28.0.dev0\"\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "alignment_config = {\n",
    "    'layer': 25,\n",
    "    \"token_range\" : [0, 1]\n",
    "}\n",
    "model = AlignableLlamaForCausalLM.from_pretrained(\n",
    "    \"../../alpaca_7b_bf16/\",\n",
    "    alignment_config=alignment_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "_ = model.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"../../alpaca_7b/\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4f8251b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt_template = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Input:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "alpaca_prompt_template_no_inputs = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c38394",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_instruction = \"\"\"Please say yes only if Sam is heavier than John, otherwise no.\"\"\"\n",
    "prompt = alpaca_prompt_template % (alpaca_instruction, \"Sam weigh 112 lbs, John weigh 163 lbs\", \"\")\n",
    "\n",
    "# alpaca_instruction = \"\"\"Does Donald Trump use computer?\"\"\"\n",
    "# prompt = alpaca_prompt_template_no_inputs % (alpaca_instruction)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(\"cuda\")\n",
    "model.eval()\n",
    "outputs = model(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask\n",
    ")\n",
    "pred_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "generated_tokens = tokenizer.decode(pred_labels[0])\n",
    "\n",
    "afc_1 = tokenizer.convert_tokens_to_ids(\"Yes\")\n",
    "afc_2 = tokenizer.convert_tokens_to_ids(\"No\")\n",
    "afc_1_prob = outputs.logits[:, -1][0][afc_1]\n",
    "afc_2_prob = outputs.logits[:, -1][0][afc_2]\n",
    "if afc_1_prob > afc_2_prob:\n",
    "    afc = \"Yes\"\n",
    "else:\n",
    "    afc = \"No\"\n",
    "print(f\"afc label = {afc} ({afc_1_prob}/{afc_2_prob}) ; pred label = {generated_tokens}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cac3d8",
   "metadata": {},
   "source": [
    "#### Working task: Pricing Tag Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5a8659bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prealign = factual_sampler(\n",
    "    tokenizer,\n",
    "    5000,\n",
    "    game=\"pricing_tag\"\n",
    ")\n",
    "prealign_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_prealign[0], \n",
    "        \"labels\": raw_prealign[1],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "prealign_dataloader = DataLoader(\n",
    "    prealign_dataset, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f6ad2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [01:23<00:00,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_count = 0\n",
    "correct_count = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(tqdm(prealign_dataloader)):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(model.device)\n",
    "\n",
    "        # aligning forward!\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            labels=inputs['labels'],\n",
    "        )\n",
    "\n",
    "        actual_test_labels = inputs['labels'][:, -1]\n",
    "        pred_test_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "\n",
    "        correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "        total_count += len(correct_labels)\n",
    "        correct_count += correct_labels.sum().tolist()\n",
    "current_acc = round(correct_count/total_count, 2)\n",
    "print(f\"[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f2df4",
   "metadata": {},
   "source": [
    "#### Working task: Pricing Tag Game with Material Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b416c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for i in range(100, 1000):\n",
    "    examples += [{\n",
    "        'a' : f\"gold {str(i)[0]}.{str(i)[1]}{str(i)[2]} dollars\",\n",
    "        'output' : ['Yes', 'yes'] if i < 700 and i > 200 else ['No', 'no']\n",
    "        # 'output' : ['No', 'no']\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"\"\"Please say yes only if wood costs between 3.00 and 4.99 dollars, otherwise no.\"\"\"\n",
    "# instruction = \"\"\"Please say yes only if metal costs between 2.00 and 6.99 dollars, otherwise no.\"\"\"\n",
    "alpaca_prompt_template = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Input:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = 0\n",
    "model.eval()\n",
    "pred_labels = []\n",
    "with torch.no_grad():\n",
    "    for example in tqdm(examples):\n",
    "        inputs = example['a']\n",
    "        alpaca_prompt = alpaca_prompt_template % (instruction, inputs)\n",
    "\n",
    "        input_ids = tokenizer(alpaca_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        generated_ids = model.generate(\n",
    "            input_ids, do_sample=True, \n",
    "            temperature=0.01, max_length=input_ids[0].shape[0]+3,\n",
    "        )\n",
    "        generated_tokens = tokenizer.decode(generated_ids[0])\n",
    "        pred_label = generated_tokens.split(\"</s>\")[0].strip(\".\\n\").split(\"\\n\")[-1].strip()\n",
    "        correct_label = example['output']\n",
    "        if pred_label in correct_label:\n",
    "            n_correct += 1\n",
    "        print(pred_label, correct_label, inputs)\n",
    "        pred_labels += [pred_label]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b642599",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct/len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d6f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_x = []\n",
    "plot_data_y = []\n",
    "for i in range(len(pred_labels)):\n",
    "    plot_data_x += [round((i+100)/100.0, 2)]\n",
    "    plot_data_y += [0 if pred_labels[i] in ['No', 'no'] else 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,1))\n",
    "plt.plot(plot_data_x, plot_data_y, '-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
