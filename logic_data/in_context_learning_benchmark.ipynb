{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6127bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d20e1",
   "metadata": {},
   "source": [
    "### Benchmark Creation\n",
    "- logic operators: ==, !=, or, and\n",
    "- arity of 3: a, b, c unique boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP = True\n",
    "FIXED = True\n",
    "if not SKIP:\n",
    "    if not FIXED:\n",
    "        for i in range(len(L)):\n",
    "            L[i] = L[i].replace('\\\"', '')\n",
    "            for key in table:\n",
    "                L[i] = L[i].replace(key, table[key])\n",
    "\n",
    "        cfg = CFG(*L)\n",
    "        primitive_clauses = set([])\n",
    "        equivalent_clauses = set([])\n",
    "        for _ in range(10000): # 10000 will get you all the programs.\n",
    "            clause = generateSentence(cfg)\n",
    "            if \"a == a\" in clause or \\\n",
    "                \"a != a\" in clause or \\\n",
    "                \"b == b\" in clause or \\\n",
    "                \"b != b\" in clause or \\\n",
    "                \"c == c\" in clause or \\\n",
    "                \"c != c\" in clause:\n",
    "                continue\n",
    "\n",
    "            if \"and\" in clause and \\\n",
    "                clause.split(\" and \")[0] == clause.split(\" and \")[1]:\n",
    "                continue\n",
    "            if \"or\" in clause and \\\n",
    "                clause.split(\" or \")[0] == clause.split(\" or \")[1]:\n",
    "                continue\n",
    "\n",
    "            if \"and\" in clause:\n",
    "                variables = set([])\n",
    "                for c in clause.split(\" and \"):\n",
    "                    if \"==\" in c:\n",
    "                        for t in c.strip(\"() \").split(\" == \"):\n",
    "                            variables.add(t)\n",
    "                    elif \"!=\" in c:\n",
    "                        for t in c.strip(\"() \").split(\" != \"):\n",
    "                            variables.add(t)\n",
    "                if len(variables) < 3:\n",
    "                    continue\n",
    "\n",
    "                left = sorted(clause.split(\" and \")[0].split())\n",
    "                right = sorted(clause.split(\" and \")[1].split())\n",
    "\n",
    "                if (\"and\", tuple(left), tuple(right)) in equivalent_clauses or \\\n",
    "                    (\"and\", tuple(right), tuple(left)) in equivalent_clauses:\n",
    "                    continue\n",
    "                else:\n",
    "                    equivalent_clauses.add((\"and\", tuple(left), tuple(right)))\n",
    "                    equivalent_clauses.add((\"and\", tuple(right), tuple(left)))\n",
    "\n",
    "            if \"or\" in clause:\n",
    "                variables = set([])\n",
    "                for c in clause.split(\" or \"):\n",
    "                    if \"==\" in c:\n",
    "                        for t in c.strip(\"() \").split(\" == \"):\n",
    "                            variables.add(t)\n",
    "                    elif \"!=\" in c:\n",
    "                        for t in c.strip(\"() \").split(\" != \"):\n",
    "                            variables.add(t)\n",
    "                if len(variables) < 3:\n",
    "                    continue\n",
    "\n",
    "                left = sorted(clause.split(\" or \")[0].split())\n",
    "                right = sorted(clause.split(\" or \")[1].split())\n",
    "\n",
    "                if (\"or\", tuple(left), tuple(right)) in equivalent_clauses or \\\n",
    "                    (\"or\", tuple(right), tuple(left)) in equivalent_clauses:\n",
    "                    continue\n",
    "                else:\n",
    "                    equivalent_clauses.add((\"or\", tuple(left), tuple(right)))\n",
    "                    equivalent_clauses.add((\"or\", tuple(right), tuple(left)))\n",
    "\n",
    "            primitive_clauses.add(clause)\n",
    "\n",
    "\n",
    "        primitive_clauses = list(primitive_clauses)\n",
    "        random.shuffle(primitive_clauses)\n",
    "        training_clauses = primitive_clauses[:20]\n",
    "        eval_clauses = primitive_clauses[20:]\n",
    "        pickle.dump(primitive_clauses, open(\"./cfg_all.pkl\", 'wb'))\n",
    "        pickle.dump(training_clauses, open(\"./cfg_train.pkl\", 'wb'))\n",
    "        pickle.dump(eval_clauses, open(\"./cfg_test.pkl\", 'wb'))\n",
    "    else:\n",
    "        primitive_clauses = [\n",
    "         '( a != c ) and ( a == b )',\n",
    "         '( c != a ) or ( b != a )',\n",
    "         '( b != a ) and ( c != a )',\n",
    "         '( b == a ) or ( c != a )',\n",
    "         '( a == b ) or ( b == c )',\n",
    "         '( c == a ) and ( a == b )',\n",
    "         '( b != a ) and ( b != c )',\n",
    "         '( b == a ) or ( a == c )',\n",
    "         '( c == a ) and ( c != b )',\n",
    "         '( c == b ) and ( c != a )',\n",
    "         '( c != b ) or ( a == c )',\n",
    "         '( b != a ) or ( c == a )',\n",
    "         '( c != b ) and ( c != a )',\n",
    "         '( a == c ) or ( b == c )',\n",
    "        ]\n",
    "        random.shuffle(primitive_clauses)\n",
    "        training_clauses = primitive_clauses[:11]\n",
    "        eval_clauses = primitive_clauses[11:]\n",
    "        pickle.dump(primitive_clauses, open(\"./cfg_all.pkl\", 'wb'))\n",
    "        pickle.dump(training_clauses, open(\"./cfg_train.pkl\", 'wb'))\n",
    "        pickle.dump(eval_clauses, open(\"./cfg_test.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a865e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_examples = 100000\n",
    "n_eval_examples = 1000\n",
    "n_test_examples = 1000\n",
    "n_training_program = 9\n",
    "n_fewshot = 10\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "training_clauses = ['( c == a ) or ( a != b )',\n",
    " '( c != a ) and ( a == b )',\n",
    " '( c == a ) or ( b != c )',\n",
    " '( c == a ) or ( b == c )',\n",
    " '( c != a ) or ( a == b )',\n",
    " '( b != a ) and ( b != c )',\n",
    " '( c != a ) or ( a != b )',\n",
    " '( c == a ) and ( b != c )',\n",
    " '( c != a ) and ( b != c )',\n",
    " '( c == a ) or ( a == b )',\n",
    " '( c != a ) and ( a != b )']\n",
    "#################\n",
    "#\n",
    "# DO NOT CHANGE\n",
    "#\n",
    "#################\n",
    "n_examples = n_fewshot + 1\n",
    "training_clauses = pickle.load(open(\"./cfg_train.pkl\", 'rb'))\n",
    "eval_clauses = pickle.load(open(\"./cfg_test.pkl\", 'rb'))\n",
    "if n_training_program is not None:\n",
    "    training_clauses = random.sample(training_clauses, k=n_training_program)\n",
    "    \n",
    "all_train_input_ids = []\n",
    "all_train_output_ids = []\n",
    "all_train_clauses = []\n",
    "m = {}\n",
    "for c in training_clauses:\n",
    "    m[c] = [0, 0]\n",
    "for i in tqdm(range(n_training_examples)):\n",
    "    clauses = random.choice(training_clauses)\n",
    "    demostrations = sample_demonstrations_for_clauses_forward(\n",
    "        clauses,\n",
    "        n_examples\n",
    "    )\n",
    "        \n",
    "    # listify\n",
    "    input_ids = [BOS_TOKEN_ID]\n",
    "    output_ids = [BOS_TOKEN_ID]\n",
    "    for d in demostrations:\n",
    "        m[clauses][int(d['output'])] += 1\n",
    "        output = FALSE_TOKEN_ID if d['output'] == False else TRUE_TOKEN_ID\n",
    "        input_ids += [INPUT_PREFIX_TOKEN_ID, d['a'], d['b'], d['c'], OUTPUT_PREFIX_TOKEN_ID, output, SEPARATOR_TOKEN_ID]\n",
    "        output_ids += [-100, -100, -100, -100, -100, output, -100]\n",
    "        assert len(input_ids) == len(output_ids)\n",
    "    input_ids += [EOS_TOKEN_ID]\n",
    "    output_ids += [EOS_TOKEN_ID]\n",
    "    all_train_input_ids += [input_ids]\n",
    "    all_train_output_ids += [output_ids]\n",
    "    all_train_clauses += [clauses]\n",
    "    \n",
    "all_eval_input_ids = []\n",
    "all_eval_output_ids = []\n",
    "all_eval_clauses = []\n",
    "for i in tqdm(range(n_eval_examples)):\n",
    "    clauses = random.choice(training_clauses)\n",
    "    clauses = '( c != b ) or ( a == c )'\n",
    "    demostrations = sample_demonstrations_for_clauses_forward(\n",
    "        clauses,\n",
    "        n_examples\n",
    "    )\n",
    "    \n",
    "    # listify\n",
    "    input_ids = [BOS_TOKEN_ID]\n",
    "    output_ids = [BOS_TOKEN_ID]\n",
    "    for d in demostrations:\n",
    "        output = FALSE_TOKEN_ID if d['output'] == False else TRUE_TOKEN_ID\n",
    "        input_ids += [INPUT_PREFIX_TOKEN_ID, d['a'], d['b'], d['c'], OUTPUT_PREFIX_TOKEN_ID, output, SEPARATOR_TOKEN_ID]\n",
    "        output_ids += [-100, -100, -100, -100, -100, output, -100]\n",
    "        assert len(input_ids) == len(output_ids)\n",
    "    input_ids += [EOS_TOKEN_ID]\n",
    "    output_ids += [EOS_TOKEN_ID]\n",
    "    all_eval_input_ids += [input_ids]\n",
    "    all_eval_output_ids += [output_ids]\n",
    "    all_eval_clauses += [clauses]\n",
    "    \n",
    "train_data = {\n",
    "    \"input_ids\" : all_train_input_ids,\n",
    "    \"output_ids\" : all_train_output_ids,\n",
    "    \"clauses\" : all_train_clauses,\n",
    "}\n",
    "dev_data = {\n",
    "    \"input_ids\" : all_eval_input_ids,\n",
    "    \"output_ids\" : all_eval_output_ids,\n",
    "    \"clauses\" : all_eval_clauses,\n",
    "}\n",
    "pickle.dump(train_data, open(f\"./train_data.n_rule.{n_training_program}.n_shot.{n_fewshot}.pkl\", 'wb'))\n",
    "pickle.dump(dev_data, open(f\"./dev_data.n_rule.{n_training_program}.n_shot.{n_fewshot}.pkl\", 'wb'))\n",
    "\n",
    "all_test_input_ids = []\n",
    "all_test_output_ids = []\n",
    "all_test_clauses = []\n",
    "for i in tqdm(range(n_test_examples)):\n",
    "    clauses = random.choice(eval_clauses)\n",
    "    demostrations = sample_demonstrations_for_clauses_forward(\n",
    "        clauses,\n",
    "        n_examples\n",
    "    )\n",
    "    \n",
    "    # listify\n",
    "    input_ids = [BOS_TOKEN_ID]\n",
    "    output_ids = [BOS_TOKEN_ID]\n",
    "    for d in demostrations:\n",
    "        output = FALSE_TOKEN_ID if d['output'] == False else TRUE_TOKEN_ID\n",
    "        input_ids += [INPUT_PREFIX_TOKEN_ID, d['a'], d['b'], d['c'], OUTPUT_PREFIX_TOKEN_ID, output, SEPARATOR_TOKEN_ID]\n",
    "        output_ids += [-100, -100, -100, -100, -100, output, -100]\n",
    "        assert len(input_ids) == len(output_ids)\n",
    "    input_ids += [EOS_TOKEN_ID]\n",
    "    output_ids += [EOS_TOKEN_ID]\n",
    "    all_test_input_ids += [input_ids]\n",
    "    all_test_output_ids += [output_ids]\n",
    "    all_test_clauses += [clauses]\n",
    "    \n",
    "test_data = {\n",
    "    \"input_ids\" : all_test_input_ids,\n",
    "    \"output_ids\" : all_test_output_ids,\n",
    "    \"clauses\" : all_test_clauses,\n",
    "}\n",
    "pickle.dump(test_data, open(f\"./test_data.n_rule.{n_training_program}.n_shot.{n_fewshot}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60076cd",
   "metadata": {},
   "source": [
    "# Bayesian Rule Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(\n",
    "    clauses,\n",
    "    assignments\n",
    "):\n",
    "    eval_clauses = clauses.replace('a ', str(assignments['a']))\n",
    "    eval_clauses = eval_clauses.replace('b ', str(assignments['b']))\n",
    "    eval_clauses= eval_clauses.replace('c ', str(assignments['c']))\n",
    "    return eval(eval_clauses)\n",
    "\n",
    "\n",
    "n_shots = 10\n",
    "n_experiment = 1000\n",
    "fig, axs = plt.subplots(2, 7, figsize=(20, 10))\n",
    "\n",
    "rule_idx = 0\n",
    "for hidden_rule in training_clauses:\n",
    "\n",
    "    experiment_results = []\n",
    "    for _ in range(n_experiment):\n",
    "        prior_hit_count = [0 for _ in training_clauses]\n",
    "        for i in range(n_shots):\n",
    "            data = sample_demonstrations_for_clauses_forward(\n",
    "                hidden_rule,\n",
    "                1\n",
    "            )[0]\n",
    "            idx = 0\n",
    "            for clauses in training_clauses:\n",
    "                eval_output = _eval(clauses, data)\n",
    "                if eval_output == data['output']:\n",
    "                    prior_hit_count[idx] += 1\n",
    "                idx += 1\n",
    "        prior_hit_count = np.array(prior_hit_count)\n",
    "        prior_hit_count = np.exp(prior_hit_count) / np.sum(np.exp(prior_hit_count))\n",
    "        experiment_results += [prior_hit_count]\n",
    "\n",
    "    # Convert the 2D list to a NumPy array\n",
    "    data = np.array(experiment_results)\n",
    "    # Calculate the mean and standard deviation of each column\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    # Set up the plot\n",
    "    \n",
    "    # Plot the mean and standard deviation as error bars\n",
    "    axs[rule_idx//7, rule_idx%7].errorbar(range(data.shape[1]), mean, yerr=std, fmt='o', capsize=5)\n",
    "    # Set the x-axis labels\n",
    "    axs[rule_idx//7, rule_idx%7].set_xticks(range(data.shape[1]))\n",
    "    axs[rule_idx//7, rule_idx%7].set_xticklabels([training_clauses[i] for i in range(data.shape[1])], rotation=90)\n",
    "    # Set the y-axis label\n",
    "    axs[rule_idx//7, rule_idx%7].set_ylabel('Value')\n",
    "    # Set the title\n",
    "    axs[rule_idx//7, rule_idx%7].set_title(f'{hidden_rule}')\n",
    "    \n",
    "    rule_idx += 1\n",
    "    \n",
    "# Show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19591804",
   "metadata": {},
   "source": [
    "## L1 & L2\n",
    "for each rule, we want to have at least one good model and one bad model, and we need to compare accuracies between them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d4917",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_clauses = ['( c == a ) or ( a != b )',\n",
    " '( c != a ) and ( a == b )',\n",
    " '( c == a ) or ( b != c )',\n",
    " '( c == a ) or ( b == c )',\n",
    " '( c != a ) or ( a == b )',\n",
    " '( b != a ) and ( b != c )',\n",
    " '( c != a ) or ( a != b )',\n",
    " '( c == a ) and ( b != c )',\n",
    " '( c != a ) and ( b != c )',\n",
    " '( c == a ) or ( a == b )',\n",
    " '( c != a ) and ( a != b )']\n",
    "include_control = False\n",
    "n_training_examples = 20000\n",
    "n_testing_examples = 1000\n",
    "shared_train = False\n",
    "n_fewshot = 10\n",
    "level = \"l1\" if shared_train else \"l2\"\n",
    "\n",
    "for clauses in training_clauses:\n",
    "    print(\"Generating for clause: \", clauses)\n",
    "    \n",
    "    n_examples = n_fewshot + 1\n",
    "    left_aligment_data = left_aligment_sampler(\n",
    "        clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train\n",
    "    )\n",
    "    left_aligment_train_data = {\n",
    "        \"base_input_ids\" : left_aligment_data[0],\n",
    "        \"base_output_ids\" : left_aligment_data[1],\n",
    "        \"source_input_ids\" : left_aligment_data[3],\n",
    "        \"source_output_ids\" : left_aligment_data[4],\n",
    "        \"counterfacut_output_ids\": left_aligment_data[6],\n",
    "        \"clauses\" : left_aligment_data[2],\n",
    "        \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]))]\n",
    "    }\n",
    "    clauses_no_space = clauses.replace(\" \", \"\")\n",
    "    pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.{level}.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "    \n",
    "    left_aligment_sampled_data = left_aligment_sampler(\n",
    "        clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    "    )\n",
    "\n",
    "    left_aligment_test_data = {\n",
    "        \"base_input_ids\" : left_aligment_sampled_data[0],\n",
    "        \"base_output_ids\" : left_aligment_sampled_data[1],\n",
    "        \"source_input_ids\" : left_aligment_sampled_data[3],\n",
    "        \"source_output_ids\" : left_aligment_sampled_data[4],\n",
    "        \"counterfacut_output_ids\": left_aligment_sampled_data[6],\n",
    "        \"clauses\" : left_aligment_sampled_data[2],\n",
    "        \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]))]\n",
    "    }\n",
    "    clauses_no_space = clauses.replace(\" \", \"\")\n",
    "    pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.{level}.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "    \n",
    "    if include_control:\n",
    "        left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "            clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train\n",
    "        )\n",
    "        left_identity_alignment_train_data = {\n",
    "            \"base_input_ids\" : left_identity_alignment_data[0],\n",
    "            \"base_output_ids\" : left_identity_alignment_data[1],\n",
    "            \"source_input_ids\" : left_identity_alignment_data[3],\n",
    "            \"source_output_ids\" : left_identity_alignment_data[4],\n",
    "            \"counterfacut_output_ids\": left_identity_alignment_data[6],\n",
    "            \"clauses\" : left_identity_alignment_data[2],\n",
    "            \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]))]\n",
    "        }\n",
    "        clauses_no_space = clauses.replace(\" \", \"\")\n",
    "        pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.{level}.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "        left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "            clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    "        )\n",
    "        left_identity_alignment_test_data = {\n",
    "            \"base_input_ids\" : left_identity_alignment_sampled_data[0],\n",
    "            \"base_output_ids\" : left_identity_alignment_sampled_data[1],\n",
    "            \"source_input_ids\" : left_identity_alignment_sampled_data[3],\n",
    "            \"source_output_ids\" : left_identity_alignment_sampled_data[4],\n",
    "            \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6],\n",
    "            \"clauses\" : left_identity_alignment_sampled_data[2],\n",
    "            \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]))]\n",
    "        }\n",
    "        clauses_no_space = clauses.replace(\" \", \"\")\n",
    "        pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.{level}.clauses.{clauses_no_space}.pkl\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a71ff",
   "metadata": {},
   "source": [
    "## L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35327377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clauses = \"( c == a ) or ( b != a )\"\n",
    "# source_clauses = \"( c == a ) and ( a == b )\"\n",
    "n_training_examples = 20000\n",
    "shared_train = False\n",
    "\n",
    "left_aligment_data = left_aligment_sampler(\n",
    "    clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "    clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_train_data = {\n",
    "    \"base_input_ids\" : left_aligment_data[0],\n",
    "    \"base_output_ids\" : left_aligment_data[1],\n",
    "    \"source_input_ids\" : left_aligment_data[3],\n",
    "    \"source_output_ids\" : left_aligment_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_data[6],\n",
    "    \"clauses\" : left_aligment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "source_clauses_no_space = source_clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_train_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_data[6],\n",
    "    \"clauses\" : left_identity_alignment_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "n_testing_examples = 1000\n",
    "left_aligment_sampled_data = left_aligment_sampler(\n",
    "    clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "    clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_test_data = {\n",
    "    \"base_input_ids\" : left_aligment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_aligment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_aligment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_aligment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_sampled_data[6],\n",
    "    \"clauses\" : left_aligment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_test_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_sampled_data[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_sampled_data[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_sampled_data[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_sampled_data[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6],\n",
    "    \"clauses\" : left_identity_alignment_sampled_data[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l3.clauses.{clauses_no_space}+{source_clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bb701",
   "metadata": {},
   "source": [
    "## L3+ (L3 Exchange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22302b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clauses = \"( a != b ) and ( c == b )\"\n",
    "# source_clauses = \"( a != b ) or ( c == a )\"\n",
    "n_training_examples = 20000\n",
    "shared_train = False\n",
    "\n",
    "left_aligment_data = left_aligment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_data = left_identity_alignment_sampler(\n",
    "    clauses, n_training_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_data_exchange = left_aligment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "left_identity_alignment_data_exchange = left_identity_alignment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "\n",
    "left_aligment_train_data = {\n",
    "    \"base_input_ids\" : left_aligment_data[0]+left_aligment_data_exchange[0],\n",
    "    \"base_output_ids\" : left_aligment_data[1]+left_aligment_data_exchange[1],\n",
    "    \"source_input_ids\" : left_aligment_data[3]+left_aligment_data_exchange[3],\n",
    "    \"source_output_ids\" : left_aligment_data[4]+left_aligment_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_data[6]+left_aligment_data_exchange[6],\n",
    "    \"clauses\" : left_aligment_data[2]+left_aligment_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_data[0]+left_aligment_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "source_clauses_no_space = source_clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_train_data, open(f\"./left_aligment_train_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_train_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_data[0]+left_identity_alignment_data_exchange[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_data[1]+left_identity_alignment_data_exchange[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_data[3]+left_identity_alignment_data_exchange[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_data[4]+left_identity_alignment_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_data[6]+left_identity_alignment_data_exchange[6],\n",
    "    \"clauses\" : left_identity_alignment_data[2]+left_identity_alignment_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_data[0]+left_identity_alignment_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_train_data, open(f\"./left_identity_alignment_train_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "n_testing_examples = 1000\n",
    "left_aligment_sampled_data = left_aligment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "    clauses, n_testing_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    ")\n",
    "\n",
    "left_aligment_sampled_data_exchange = left_aligment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "left_identity_alignment_sampled_data_exchange = left_identity_alignment_sampler(\n",
    "    source_clauses, n_training_examples, shared_train=shared_train, source_clauses=clauses\n",
    ")\n",
    "\n",
    "left_aligment_test_data = {\n",
    "    \"base_input_ids\" : left_aligment_sampled_data[0]+left_aligment_sampled_data_exchange[0],\n",
    "    \"base_output_ids\" : left_aligment_sampled_data[1]+left_aligment_sampled_data_exchange[1],\n",
    "    \"source_input_ids\" : left_aligment_sampled_data[3]+left_aligment_sampled_data_exchange[3],\n",
    "    \"source_output_ids\" : left_aligment_sampled_data[4]+left_aligment_sampled_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_aligment_sampled_data[6]+left_aligment_sampled_data_exchange[6],\n",
    "    \"clauses\" : left_aligment_sampled_data[2]+left_aligment_sampled_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]+left_aligment_sampled_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "left_identity_alignment_test_data = {\n",
    "    \"base_input_ids\" : left_identity_alignment_sampled_data[0]+left_identity_alignment_sampled_data_exchange[0],\n",
    "    \"base_output_ids\" : left_identity_alignment_sampled_data[1]+left_identity_alignment_sampled_data_exchange[1],\n",
    "    \"source_input_ids\" : left_identity_alignment_sampled_data[3]+left_identity_alignment_sampled_data_exchange[3],\n",
    "    \"source_output_ids\" : left_identity_alignment_sampled_data[4]+left_identity_alignment_sampled_data_exchange[4],\n",
    "    \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6]+left_identity_alignment_sampled_data_exchange[6],\n",
    "    \"clauses\" : left_identity_alignment_sampled_data[2]+left_identity_alignment_sampled_data_exchange[2],\n",
    "    \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]+left_identity_alignment_sampled_data_exchange[0]))]\n",
    "}\n",
    "clauses_no_space = clauses.replace(\" \", \"\")\n",
    "pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l3.clauses.{clauses_no_space}<>{source_clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387e505",
   "metadata": {},
   "source": [
    "## L1 Unrolling Eval Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83496b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clauses = '( c != a ) or ( b == a )'\n",
    "\n",
    "shared_train = True\n",
    "for n_examples in range(11, 12):\n",
    "\n",
    "    n_testing_examples = 1000\n",
    "    left_aligment_sampled_data = left_aligment_sampler(\n",
    "        clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    "    )\n",
    "    left_identity_alignment_sampled_data = left_identity_alignment_sampler(\n",
    "        clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    "    )\n",
    "\n",
    "    left_aligment_test_data = {\n",
    "        \"base_input_ids\" : left_aligment_sampled_data[0],\n",
    "        \"base_output_ids\" : left_aligment_sampled_data[1],\n",
    "        \"source_input_ids\" : left_aligment_sampled_data[3],\n",
    "        \"source_output_ids\" : left_aligment_sampled_data[4],\n",
    "        \"counterfacut_output_ids\": left_aligment_sampled_data[6],\n",
    "        \"clauses\" : left_aligment_sampled_data[2],\n",
    "        \"intervention_ids\": [0 for i in range(len(left_aligment_sampled_data[0]))]\n",
    "    }\n",
    "    clauses_no_space = clauses.replace(\" \", \"\")\n",
    "    pickle.dump(left_aligment_test_data, open(f\"./left_aligment_test_data.l1.unrolling.{n_examples-1}.clauses.{clauses_no_space}.pkl\", 'wb'))\n",
    "\n",
    "    left_identity_alignment_test_data = {\n",
    "        \"base_input_ids\" : left_identity_alignment_sampled_data[0],\n",
    "        \"base_output_ids\" : left_identity_alignment_sampled_data[1],\n",
    "        \"source_input_ids\" : left_identity_alignment_sampled_data[3],\n",
    "        \"source_output_ids\" : left_identity_alignment_sampled_data[4],\n",
    "        \"counterfacut_output_ids\": left_identity_alignment_sampled_data[6],\n",
    "        \"clauses\" : left_identity_alignment_sampled_data[2],\n",
    "        \"intervention_ids\": [0 for i in range(len(left_identity_alignment_sampled_data[0]))]\n",
    "    }\n",
    "    clauses_no_space = clauses.replace(\" \", \"\")\n",
    "    pickle.dump(left_identity_alignment_test_data, open(f\"./left_identity_alignment_test_data.l1.unrolling.{n_examples-1}.clauses.{clauses_no_space}.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285a974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca8bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
