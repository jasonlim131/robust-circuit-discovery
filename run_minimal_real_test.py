#!/usr/bin/env python3
"""
Minimal test that actually works - no fake results
"""

import torch
import json
from pyvene import create_gpt2, IntervenableConfig, RepresentationConfig, IntervenableModel
from ioi_utils_simplified import PromptDistribution, NAMES, OBJECTS, PLACES, TEMPLATES

def run_minimal_real_test():
    """Run minimal test that actually works"""
    print("üî¨ MINIMAL REAL TEST - No Fake Results")
    print("=" * 50)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    # Load model
    config, tokenizer, model = create_gpt2("gpt2")
    model = model.to(device)
    
    # Set padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("‚úÖ GPT-2 loaded successfully")
    
    # Create simple test data
    distribution = PromptDistribution(
        names=NAMES[:10],
        objects=OBJECTS[:5],
        places=PLACES[:5], 
        templates=TEMPLATES[:2]
    )
    
    # Generate very simple IOI examples
    base_prompts = []
    for _ in range(3):
        prompt = distribution.sample_one("ABB")  # John, Mary, John pattern
        base_prompts.append(prompt.sentence)
    
    print(f"\nüìù Test prompts:")
    for i, prompt in enumerate(base_prompts):
        print(f"{i+1}. {prompt}")
    
    # Test basic model behavior (no intervention)
    print(f"\nüß™ Testing baseline model behavior...")
    
    with torch.no_grad():
        for i, prompt in enumerate(base_prompts):
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            outputs = model(**inputs)
            logits = outputs.logits[0, -1, :]  # Last token logits
            
            # Get top predictions
            top_tokens = torch.topk(logits, 5)
            top_words = [tokenizer.decode(token_id) for token_id in top_tokens.indices]
            top_probs = torch.softmax(top_tokens.values, dim=0)
            
            print(f"\nPrompt {i+1}: '{prompt}'")
            print("Top predictions:")
            for word, prob in zip(top_words, top_probs):
                print(f"  '{word}': {prob:.3f}")
    
    # Try simple intervention (if this works, we can build up)
    print(f"\nüîß Testing simple intervention...")
    
    try:
        # Create simple intervention config
        config = IntervenableConfig([
            RepresentationConfig(
                layer=6,
                component="block_output",
                unit="pos",
                low_rank_dimension=1
            )
        ])
        
        intervenable = IntervenableModel(config, model)
        intervenable.set_device(device)
        intervenable.disable_model_gradients()
        
        # Test on one prompt
        test_prompt = base_prompts[0]
        base_inputs = tokenizer(test_prompt, return_tensors="pt").to(device)
        source_inputs = tokenizer(test_prompt, return_tensors="pt").to(device)  # Same as base for now
        
        print(f"Testing intervention on: '{test_prompt}'")
        
        # Try intervention
        _, counterfactual = intervenable(
            base_inputs,
            [source_inputs],
            {"sources->base": ([[[0]]], [[[0]]])}  # Intervene on first position
        )
        
        # Compare outputs
        baseline_logits = model(**base_inputs).logits[0, -1, :]
        intervention_logits = counterfactual.logits[0, -1, :]
        
        # Calculate difference
        logit_diff = torch.abs(intervention_logits - baseline_logits).mean()
        
        print(f"‚úÖ Intervention successful!")
        print(f"Average logit difference: {logit_diff:.4f}")
        
        if logit_diff > 0.001:
            print(f"üéØ Intervention had measurable effect: {logit_diff:.4f}")
            intervention_successful = True
        else:
            print(f"‚ö†Ô∏è Intervention effect very small: {logit_diff:.4f}")
            intervention_successful = False
            
    except Exception as e:
        print(f"‚ùå Intervention failed: {e}")
        intervention_successful = False
        logit_diff = 0.0
    
    # Create REAL results (no fakes)
    real_results = {
        "experiment_type": "minimal_real_test",
        "model": "GPT-2 (124M)",
        "device": device,
        "test_prompts": base_prompts,
        "baseline_testing": "successful",
        "intervention_testing": "successful" if intervention_successful else "failed",
        "intervention_effect": float(logit_diff) if intervention_successful else 0.0,
        "technical_issues": [] if intervention_successful else ["Tensor dimension errors in complex interventions"],
        "system_status": "partial_functionality" if intervention_successful else "intervention_issues",
        "honest_assessment": {
            "what_works": [
                "GPT-2 model loading",
                "Tokenization and basic inference", 
                "Simple intervention framework setup"
            ],
            "what_needs_fixing": [
                "Attention head specific interventions",
                "Position indexing in interventions",
                "Batch processing with padding"
            ] if not intervention_successful else ["Need to test more complex interventions"],
            "next_steps": [
                "Fix tensor dimension issues",
                "Debug position indexing", 
                "Test individual attention heads",
                "Scale up to full discovery pipeline"
            ]
        }
    }
    
    # Save honest results
    with open("honest_minimal_results.json", "w") as f:
        json.dump(real_results, f, indent=2)
    
    print(f"\nüìä HONEST RESULTS SUMMARY")
    print("=" * 30)
    print(f"Model loading: ‚úÖ SUCCESS")
    print(f"Basic inference: ‚úÖ SUCCESS") 
    print(f"Simple intervention: {'‚úÖ SUCCESS' if intervention_successful else '‚ùå FAILED'}")
    print(f"Complex discovery: ‚ùå NOT TESTED (issues to fix first)")
    
    print(f"\nüíæ Honest results saved to: honest_minimal_results.json")
    print(f"\nüéØ CONCLUSION: System has potential but needs debugging before full discovery")
    
    return real_results

if __name__ == "__main__":
    results = run_minimal_real_test()