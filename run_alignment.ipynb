{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05aad150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import *\n",
    "from logic_data.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2708fbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in a notebook env.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: The testing components of [-h] [--gpu GPU]\n",
      "                                 [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                                 [--eval_batch_size EVAL_BATCH_SIZE] [--lr LR]\n",
      "                                 --data_path DATA_PATH --train_data_path\n",
      "                                 TRAIN_DATA_PATH --test_data_path\n",
      "                                 TEST_DATA_PATH\n",
      "                                 [--encoder_config_path ENCODER_CONFIG_PATH]\n",
      "                                 [--decoder_config_path DECODER_CONFIG_PATH]\n",
      "                                 [--max_seq_len MAX_SEQ_LEN] [--seed SEED]\n",
      "                                 [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                                 --output_dir OUTPUT_DIR\n",
      "                                 [--local_rank LOCAL_RANK] [--epochs EPOCHS]\n",
      "                                 [--model_path MODEL_PATH] [--warm_up WARM_UP]\n",
      "                                 [--is_wandb] [--log_step LOG_STEP]\n",
      "                                 [--valid_steps VALID_STEPS]\n",
      "                                 [--early_stopping EARLY_STOPPING]\n",
      "                                 [--device DEVICE] [--do_prealign_eval]\n",
      "                                 [--do_align] [--do_eval] [--do_test]\n",
      "                                 [--n_training_program N_TRAINING_PROGRAM]\n",
      "                                 [--n_fewshot N_FEWSHOT]\n",
      "                                 [--aligning_layer_n ALIGNING_LAYER_N]\n",
      "The testing components of: error: the following arguments are required: --data_path, --train_data_path, --test_data_path, --output_dir\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    is_notebook = False\n",
    "    try:\n",
    "        cmd = argparse.ArgumentParser('The testing components of')\n",
    "        cmd.add_argument('--gpu', default=-1, type=int, help='use id of gpu, -1 if cpu.')\n",
    "        cmd.add_argument('--train_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--eval_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--lr', default=0.01, type=float, help='learning rate')\n",
    "        cmd.add_argument('--data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument('--train_data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument('--test_data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument(\n",
    "            '--encoder_config_path', \n",
    "            type=str, help='path to the encoder config'\n",
    "        )\n",
    "        cmd.add_argument(\n",
    "            '--decoder_config_path', \n",
    "            type=str, help='path to the decoder config'\n",
    "        )\n",
    "        cmd.add_argument('--max_seq_len', default=512, type=int)\n",
    "        cmd.add_argument('--seed', default=42, type=int)\n",
    "        cmd.add_argument('--gradient_accumulation_steps', default=1, type=int)\n",
    "        cmd.add_argument('--output_dir', required=True, type=str, help='save dir')\n",
    "        cmd.add_argument('--local_rank', default=-1, type=int, help='multi gpu training')\n",
    "        cmd.add_argument('--epochs', default=10, type=int, help='training epochs')\n",
    "        cmd.add_argument('--model_path', type=str, required=False, default=None)\n",
    "        cmd.add_argument('--warm_up', type=float, default=0.1)\n",
    "        cmd.add_argument('--is_wandb', default=False, action='store_true')\n",
    "        cmd.add_argument('--log_step', default=10, type=int)\n",
    "        cmd.add_argument('--valid_steps', default=500, type=int)\n",
    "        cmd.add_argument('--early_stopping', default=5, type=int)\n",
    "        cmd.add_argument('--device', default=\"cuda\", type=str, help='')\n",
    "        cmd.add_argument('--do_prealign_eval', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_align', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_eval', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_test', default=False, action='store_true')\n",
    "        \n",
    "        cmd.add_argument('--n_training_program', default=5, type=int)\n",
    "        cmd.add_argument('--n_fewshot', default=6, type=int)\n",
    "        cmd.add_argument('--aligning_layer_n', default=0, type=int)\n",
    "        \n",
    "        args = cmd.parse_args(sys.argv[1:])\n",
    "    except:\n",
    "        is_notebook = True\n",
    "        parser = argparse.ArgumentParser()\n",
    "        args = parser.parse_args([])\n",
    "        args.gpu = 1\n",
    "        args.train_batch_size = 64\n",
    "        args.eval_batch_size = 64\n",
    "        args.gradient_accumulation_steps = 2\n",
    "        args.lr = 1e-4\n",
    "        args.data_path = \"./logic_data\"\n",
    "        args.train_data_path = \"\"\n",
    "        args.test_data_path = \"\"\n",
    "        args.encoder_config_path = None\n",
    "        args.decoder_config_path = None\n",
    "        args.max_seq_len = 512\n",
    "        args.seed = 66\n",
    "        args.output_dir = \"./results_notebook/\"\n",
    "        args.epochs = 10\n",
    "        args.warm_up = 0.1\n",
    "        args.is_wandb = False\n",
    "        args.log_step = 10\n",
    "        args.valid_steps = 100 # -1 not do training eval!\n",
    "        args.early_stopping = 999 # large == never early stop!\n",
    "        args.device = \"cuda:0\"\n",
    "        args.do_prealign_eval = True # do it once at least!\n",
    "        args.do_align = True\n",
    "        args.do_eval = True\n",
    "        args.do_test = True\n",
    "        args.model_path = \"./results_notebook/logic_pipeline.model.gpt2.n_rule.9.n_shot.10.seed.66/model-last/\"\n",
    "        # args.model_path = None\n",
    "        \n",
    "        # alignment search setting\n",
    "        args.aligning_layer_n = 0\n",
    "        args.aligning_basis_n_per_variable = 600\n",
    "        args.aligning_var_n = 1\n",
    "        \n",
    "        print(\"Using in a notebook env.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8aab211",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e641e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligning for clauses:  ( c != a ) or ( a == b )\n",
      "seed:  66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 3822.37it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4152.85it/s]\n",
      "INFO:root:intervention_config = {0: [[0, 600]]}\n",
      "INFO:root:alignment_config = {'layer': 0, 'token_range': [71, 75]}\n",
      "INFO:root:Loading pretrained model.\n",
      "INFO:root:__Number CUDA Devices: 1\n",
      "Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 12.14it/s, acc=0.99]\n",
      "INFO:root:Number of logic_pipeline.model.gpt2.n_rule.9.n_shot.10.seed.66 model params: 1440000\n",
      "INFO:root:OUTPUT DIR: ./results_notebook/logic_pipeline.model.gpt2.n_rule.9.n_shot.10.seed.66.clauses.(c!=a)or(a==b).align.left_aligment.seed.66\n",
      "Epoch: 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.26it/s, loss=1.25]\n",
      "Epoch: 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.36it/s, loss=1.17]\n",
      "Epoch: 2: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.45it/s, loss=1.34]\n",
      "Epoch: 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.40it/s, loss=0]\n",
      "Epoch: 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.27it/s, loss=0]\n",
      "Epoch: 5: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.28it/s, loss=0.02]\n",
      "Epoch: 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.13it/s, loss=0]\n",
      "Epoch: 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.24it/s, loss=0]\n",
      "Epoch: 8: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.21it/s, loss=0.04]\n",
      "Epoch: 9: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:09<00:00,  8.34it/s, loss=0.07]\n",
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [01:35<00:00,  9.53s/it]\n",
      "INFO:root:Training is finished ...\n",
      "Iteration: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 12.20it/s, acc=0.99]\n"
     ]
    }
   ],
   "source": [
    "training_clauses = [\n",
    "    '( c != a ) or ( a == b )',\n",
    "#     '( c == a ) or ( a != b )',\n",
    "#     '( c != a ) and ( a == b )',\n",
    "#     '( c == a ) or ( b != c )',\n",
    "#     '( c == a ) or ( b == c )',\n",
    "#     '( b != a ) and ( b != c )',\n",
    "#     '( c != a ) or ( a != b )',\n",
    "#     '( c == a ) and ( b != c )',\n",
    "#     '( c != a ) and ( b != c )',\n",
    "#     '( c == a ) or ( a == b )',\n",
    "#     '( c != a ) and ( a != b )'\n",
    "]\n",
    "align_type = \"left_aligment\"\n",
    "if align_type == \"function_aligment\":\n",
    "    source_clauses_pool = [\n",
    "        '( c != a ) or ( a != b )',\n",
    "        '( c == a ) and ( b != c )',\n",
    "        '( c != a ) and ( b != c )',\n",
    "        '( c == a ) or ( a == b )',\n",
    "         '( c != a ) and ( a != b )',\n",
    "        '( c != a ) or ( a == b )',\n",
    "        '( c == a ) or ( a != b )',\n",
    "        '( c != a ) and ( a == b )',\n",
    "        '( c == a ) or ( b != c )',\n",
    "        '( c == a ) or ( b == c )',\n",
    "        '( b != a ) and ( b != c )',\n",
    "    ]\n",
    "else:\n",
    "    source_clauses_pool = [\n",
    "        '( c != a ) or ( a == b )'\n",
    "    ]\n",
    "level = \"l2\"\n",
    "shared_train = True if level == \"l1\" else False\n",
    "n_fewshot = 10\n",
    "n_examples = n_fewshot + 1\n",
    "n_training_examples = 5000\n",
    "n_testing_examples = 1000\n",
    "saving_models = True\n",
    "\n",
    "for seed in [66,]: # [42, 66, 77, 88, 99]\n",
    "    args.seed = seed\n",
    "    for clauses in training_clauses:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "        clauses_no_space = clauses.replace(\" \", \"\")\n",
    "        print(\"aligning for clauses: \", clauses)\n",
    "        print(\"seed: \", seed)\n",
    "        \n",
    "        for source_clauses in source_clauses_pool:\n",
    "        # we generate the dataset onfly\n",
    "            if align_type == \"left_aligment\":\n",
    "                sampler_func = left_aligment_sampler\n",
    "                source_clauses = None\n",
    "            elif align_type == \"whole_aligment\":\n",
    "                sampler_func = whole_aligment_sampler\n",
    "                print(\"aligning with source clauses: \", source_clauses)\n",
    "            elif align_type == \"function_aligment\":\n",
    "                sampler_func = function_aligment_sampler\n",
    "                print(\"aligning with source clauses: \", source_clauses)\n",
    "\n",
    "            aligment_data = sampler_func(\n",
    "                clauses, n_training_examples, n_examples = n_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    "            )\n",
    "            train_data = {\n",
    "                \"base_input_ids\" : aligment_data[0],\n",
    "                \"base_output_ids\" : aligment_data[1],\n",
    "                \"source_input_ids\" : aligment_data[3],\n",
    "                \"source_output_ids\" : aligment_data[4],\n",
    "                \"counterfacut_output_ids\": aligment_data[6],\n",
    "                \"clauses\" : aligment_data[2],\n",
    "                \"intervention_ids\": [0 for i in range(len(aligment_data[0]))]\n",
    "            }\n",
    "            aligment_sampled_data = sampler_func(\n",
    "                clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train, source_clauses=source_clauses\n",
    "            )\n",
    "            test_data = {\n",
    "                \"base_input_ids\" : aligment_sampled_data[0],\n",
    "                \"base_output_ids\" : aligment_sampled_data[1],\n",
    "                \"source_input_ids\" : aligment_sampled_data[3],\n",
    "                \"source_output_ids\" : aligment_sampled_data[4],\n",
    "                \"counterfacut_output_ids\": aligment_sampled_data[6],\n",
    "                \"clauses\" : aligment_sampled_data[2],\n",
    "                \"intervention_ids\": [0 for i in range(len(aligment_sampled_data[0]))]\n",
    "            }\n",
    "\n",
    "            model_name = args.model_path.strip(\"/\").split(\"/\")[-2]\n",
    "            run_name = f\"{model_name}.clauses.{clauses_no_space}.align.{align_type}.seed.{args.seed}\"\n",
    "            logger = logging.getLogger()\n",
    "\n",
    "            train_dataset = Dataset.from_dict(\n",
    "                {\n",
    "                    \"input_ids\": train_data[\"base_input_ids\"], \n",
    "                    \"labels\": train_data[\"base_output_ids\"],\n",
    "                    \"source_input_ids\": train_data[\"source_input_ids\"], \n",
    "                    \"counterfactual_labels\": train_data[\"counterfacut_output_ids\"],\n",
    "                    \"intervention_ids\": train_data[\"intervention_ids\"],\n",
    "                }\n",
    "            ).with_format(\"torch\")\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                {\n",
    "                    \"input_ids\": test_data[\"base_input_ids\"], \n",
    "                    \"labels\": test_data[\"base_output_ids\"],\n",
    "                    \"source_input_ids\": test_data[\"source_input_ids\"], \n",
    "                    \"counterfactual_labels\": test_data[\"counterfacut_output_ids\"],\n",
    "                    \"intervention_ids\": test_data[\"intervention_ids\"],\n",
    "                }\n",
    "            ).with_format(\"torch\")\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=args.eval_batch_size)\n",
    "\n",
    "            # Model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            configuration = GPT2Config.from_pretrained(os.path.join(args.data_path, \"decoder_config.json\"))\n",
    "\n",
    "            if \"logic\" in model_name:\n",
    "                arity = 3\n",
    "            \n",
    "            # the following lines are for the neuron population.\n",
    "            start_idx = 1 + (arity + 4) * int(n_fewshot)\n",
    "            end_idx = start_idx + (arity+1)\n",
    "            alignment_config = {\n",
    "                \"layer\" : args.aligning_layer_n,\n",
    "                \"token_range\" : [start_idx, end_idx] # this is kind of fixed?\n",
    "            }\n",
    "            \n",
    "            if args.aligning_var_n == 1:\n",
    "                intervention_config = {\n",
    "                    0: [[0, args.aligning_basis_n_per_variable]]\n",
    "                }\n",
    "            elif args.aligning_var_n == 2:\n",
    "                intervention_config = {\n",
    "                    0: [[0, args.aligning_basis_n_per_variable]],\n",
    "                    1: [[args.aligning_basis_n_per_variable, 2*args.aligning_basis_n_per_variable]],\n",
    "                    2: [[0, args.aligning_basis_n_per_variable], [args.aligning_basis_n_per_variable, 2*args.aligning_basis_n_per_variable]]\n",
    "                }\n",
    "            logging.info(f\"intervention_config = {intervention_config}\")\n",
    "            logging.info(f\"alignment_config = {alignment_config}\")\n",
    "\n",
    "            model = AlignableGPT2LMHeadModel(configuration, alignment_config=alignment_config)\n",
    "            if args.model_path is not None:\n",
    "                logging.info(\"Loading pretrained model.\")\n",
    "                raw_weights = torch.load(os.path.join(args.model_path, 'pytorch_model.bin'))\n",
    "                model.load_state_dict(raw_weights, strict=False)\n",
    "\n",
    "            # we need to set off gradients!\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"rotate_layer\" not in name:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            device = torch.device(args.device)\n",
    "            if \"cuda:\" not in args.device:\n",
    "                n_gpu = torch.cuda.device_count()\n",
    "                logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "            else:\n",
    "                n_gpu = 1\n",
    "                logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "\n",
    "            if n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "            _ = model.to(device)\n",
    "\n",
    "            t_total = int(len(train_dataloader) * args.epochs)\n",
    "\n",
    "            warm_up_steps = args.warm_up * t_total\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                model.parameters(), lr=args.lr\n",
    "            )\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warm_up_steps,\n",
    "                                                        num_training_steps=t_total)\n",
    "            is_master = True                                    \n",
    "            if not os.path.exists(args.output_dir) and is_master:\n",
    "                os.mkdir(args.output_dir)\n",
    "            os.environ[\"WANDB_PROJECT\"] = f\"ToM-DAS\"\n",
    "\n",
    "            output_dir = os.path.join(args.output_dir, run_name)\n",
    "            if args.do_align and args.is_wandb:\n",
    "                import wandb\n",
    "                run = wandb.init(\n",
    "                    project=\"ToM-DAS-GPT2\", \n",
    "                    entity=\"wuzhengx\",\n",
    "                    name=run_name,\n",
    "                )\n",
    "                wandb.config.update(args)\n",
    "            if not os.path.exists(args.output_dir) and is_master:\n",
    "                os.mkdir(args.output_dir)\n",
    "\n",
    "            prealign_acc = None\n",
    "            test_acc = None\n",
    "\n",
    "            if args.do_prealign_eval:\n",
    "                # before doing alignment, we need to check factual performance on the dataset.\n",
    "                total_count = 0\n",
    "                correct_count = 0\n",
    "                if args.do_eval:\n",
    "                    _ = model.eval()\n",
    "                    epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "                    for step, inputs in enumerate(epoch_iterator):\n",
    "                        input_ids = inputs['input_ids'].to(device)\n",
    "                        labels = inputs['labels'].to(device)\n",
    "                        outputs = model(input_ids=input_ids)\n",
    "                        actual_test_labels = labels[:, -3]\n",
    "                        pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "                        correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "                        total_count += len(correct_labels)\n",
    "                        correct_count += correct_labels.sum().tolist()\n",
    "\n",
    "                        current_acc = round(correct_count/total_count, 2)\n",
    "                        epoch_iterator.set_postfix({'acc': current_acc})\n",
    "                prealign_acc = round(correct_count/total_count, 2)\n",
    "\n",
    "            aligner = LogicSolverAligner(\n",
    "                model, device=device, \n",
    "                logger=logger,\n",
    "                is_master=saving_models, # making it False will prevent it from saving models! \n",
    "                n_gpu=n_gpu,\n",
    "                is_wandb=args.is_wandb, \n",
    "                model_name=model_name,\n",
    "                intervention_config=intervention_config\n",
    "            )\n",
    "            num_params = count_parameters(model)\n",
    "            logging.info(f'Number of {model_name} model params: {num_params}') \n",
    "\n",
    "            # Train\n",
    "            if args.do_align:\n",
    "                logging.info(f\"OUTPUT DIR: {output_dir}\")\n",
    "                aligner.train(\n",
    "                    train_dataloader, test_dataloader,\n",
    "                    optimizer, scheduler, \n",
    "                    log_step=args.log_step, valid_steps=args.valid_steps,\n",
    "                    output_dir=output_dir, epochs=args.epochs, \n",
    "                    gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "                )\n",
    "\n",
    "            if args.is_wandb:\n",
    "                wandb.finish()\n",
    "\n",
    "            # Test\n",
    "            if args.do_test: \n",
    "                # Model\n",
    "                torch.cuda.empty_cache()\n",
    "                total_count = 0\n",
    "                correct_count = 0\n",
    "                aligner.model.eval()\n",
    "                epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "                for step, inputs in enumerate(epoch_iterator):\n",
    "                    for k, v in inputs.items():\n",
    "                        if v is not None and isinstance(v, torch.Tensor):\n",
    "                            inputs[k] = v.to(device)\n",
    "                    if aligner.preload_intervention_corr is not None:\n",
    "                        intervention_corr = aligner.preload_intervention_corr.expand(\n",
    "                            inputs['input_ids'].shape[0],-1\n",
    "                        ).to(device)\n",
    "                    else:\n",
    "                        assert False # not implemented\n",
    "\n",
    "                    # aligning forward!\n",
    "                    source_hidden_states = aligner.model(\n",
    "                       input_ids=inputs['source_input_ids']\n",
    "                    ).rotated_hidden_states\n",
    "                    outputs = aligner.model(\n",
    "                        input_ids=inputs['input_ids'],\n",
    "                        source_hidden_states=source_hidden_states,\n",
    "                        intervention_corr=intervention_corr,\n",
    "                        labels=inputs['counterfactual_labels']\n",
    "                    )\n",
    "\n",
    "                    actual_test_labels = inputs['counterfactual_labels'][:, -3]\n",
    "                    pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "                    correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "                    total_count += len(correct_labels)\n",
    "                    correct_count += correct_labels.sum().tolist()\n",
    "\n",
    "                    current_acc = round(correct_count/total_count, 2)\n",
    "                    epoch_iterator.set_postfix({'acc': current_acc})\n",
    "                test_acc = round(correct_count/total_count, 2)\n",
    "            results += [[clauses, source_clauses, seed, prealign_acc, test_acc]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74401ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results, columns = [\n",
    "    'clauses', 'source_clauses', 'seed', 'prealign_acc', 'test_acc'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3d0d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_02fe3_row0_col0, #T_02fe3_row0_col1, #T_02fe3_row0_col2, #T_02fe3_row0_col3, #T_02fe3_row1_col0, #T_02fe3_row1_col1, #T_02fe3_row1_col2, #T_02fe3_row1_col3, #T_02fe3_row2_col0, #T_02fe3_row2_col1, #T_02fe3_row2_col2, #T_02fe3_row2_col3, #T_02fe3_row3_col0, #T_02fe3_row3_col1, #T_02fe3_row3_col2, #T_02fe3_row3_col3, #T_02fe3_row4_col0, #T_02fe3_row4_col1, #T_02fe3_row4_col2, #T_02fe3_row4_col3, #T_02fe3_row5_col0, #T_02fe3_row5_col1, #T_02fe3_row5_col2, #T_02fe3_row5_col3, #T_02fe3_row6_col0, #T_02fe3_row6_col1, #T_02fe3_row6_col2, #T_02fe3_row6_col3, #T_02fe3_row7_col0, #T_02fe3_row7_col1, #T_02fe3_row7_col2, #T_02fe3_row7_col3, #T_02fe3_row8_col0, #T_02fe3_row8_col1, #T_02fe3_row8_col2, #T_02fe3_row8_col3, #T_02fe3_row9_col0, #T_02fe3_row9_col1, #T_02fe3_row9_col2, #T_02fe3_row9_col3, #T_02fe3_row10_col0, #T_02fe3_row10_col1, #T_02fe3_row10_col2, #T_02fe3_row10_col3 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_02fe3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_02fe3_level0_col0\" class=\"col_heading level0 col0\" >clauses</th>\n",
       "      <th id=\"T_02fe3_level0_col1\" class=\"col_heading level0 col1\" >source_clauses</th>\n",
       "      <th id=\"T_02fe3_level0_col2\" class=\"col_heading level0 col2\" >prealign_acc</th>\n",
       "      <th id=\"T_02fe3_level0_col3\" class=\"col_heading level0 col3\" >test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_02fe3_row0_col0\" class=\"data row0 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row0_col1\" class=\"data row0 col1\" >( b != a ) and ( b != c )</td>\n",
       "      <td id=\"T_02fe3_row0_col2\" class=\"data row0 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row0_col3\" class=\"data row0 col3\" >0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_02fe3_row1_col0\" class=\"data row1 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row1_col1\" class=\"data row1 col1\" >( c != a ) and ( a != b )</td>\n",
       "      <td id=\"T_02fe3_row1_col2\" class=\"data row1 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row1_col3\" class=\"data row1 col3\" >0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_02fe3_row2_col0\" class=\"data row2 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row2_col1\" class=\"data row2 col1\" >( c != a ) and ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row2_col2\" class=\"data row2 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row2_col3\" class=\"data row2 col3\" >0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_02fe3_row3_col0\" class=\"data row3 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row3_col1\" class=\"data row3 col1\" >( c != a ) and ( b != c )</td>\n",
       "      <td id=\"T_02fe3_row3_col2\" class=\"data row3 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row3_col3\" class=\"data row3 col3\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_02fe3_row4_col0\" class=\"data row4 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row4_col1\" class=\"data row4 col1\" >( c != a ) or ( a != b )</td>\n",
       "      <td id=\"T_02fe3_row4_col2\" class=\"data row4 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row4_col3\" class=\"data row4 col3\" >0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_02fe3_row5_col0\" class=\"data row5 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row5_col1\" class=\"data row5 col1\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row5_col2\" class=\"data row5 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row5_col3\" class=\"data row5 col3\" >0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_02fe3_row6_col0\" class=\"data row6 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row6_col1\" class=\"data row6 col1\" >( c == a ) and ( b != c )</td>\n",
       "      <td id=\"T_02fe3_row6_col2\" class=\"data row6 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row6_col3\" class=\"data row6 col3\" >0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_02fe3_row7_col0\" class=\"data row7 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row7_col1\" class=\"data row7 col1\" >( c == a ) or ( a != b )</td>\n",
       "      <td id=\"T_02fe3_row7_col2\" class=\"data row7 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row7_col3\" class=\"data row7 col3\" >0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_02fe3_row8_col0\" class=\"data row8 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row8_col1\" class=\"data row8 col1\" >( c == a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row8_col2\" class=\"data row8 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row8_col3\" class=\"data row8 col3\" >0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_02fe3_row9_col0\" class=\"data row9 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row9_col1\" class=\"data row9 col1\" >( c == a ) or ( b != c )</td>\n",
       "      <td id=\"T_02fe3_row9_col2\" class=\"data row9 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row9_col3\" class=\"data row9 col3\" >0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02fe3_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_02fe3_row10_col0\" class=\"data row10 col0\" >( c != a ) or ( a == b )</td>\n",
       "      <td id=\"T_02fe3_row10_col1\" class=\"data row10 col1\" >( c == a ) or ( b == c )</td>\n",
       "      <td id=\"T_02fe3_row10_col2\" class=\"data row10 col2\" >0.990000</td>\n",
       "      <td id=\"T_02fe3_row10_col3\" class=\"data row10 col3\" >0.760000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f05ef917e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.groupby(\n",
    "    ['clauses', 'source_clauses'], as_index=False\n",
    ").mean()[['clauses', 'source_clauses', 'prealign_acc', 'test_acc']].style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a1c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
