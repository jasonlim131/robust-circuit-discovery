{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6c7e19",
   "metadata": {},
   "source": [
    "# Introduction to pyvene\n",
    "This tutorial shows simple runnable code snippets of how to do different kinds of interventions on neural networks with pyvene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6994fa",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stanfordnlp/pyvene/blob/main/pyvene_101.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d123a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Zhengxuan Wu\"\n",
    "__version__ = \"01/20/2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26298448-91eb-4cad-85bf-ec5fef436e1d",
   "metadata": {},
   "source": [
    " # Table of Contents  \n",
    "1. [Set-up](#Set-up)     \n",
    "1. [pyvene 101](#pyvene-101) \n",
    "    1. [Set Activations to Zeros](#Set-Activation-to-Zeros) \n",
    "    1. [Set Activations with Subspaces](#Set-Activations-to-Zeros-with-Subspaces)\n",
    "    1. [Interchange Intervention](#Interchange-Interventions)\n",
    "    1. [Intervention Config](#Intervention-Configuration)\n",
    "    1. [Addition Intervention](#Addition-Intervention)\n",
    "    1. [Trainable Intervention](#Trainable-Intervention)\n",
    "    1. [Activation Collection](#Activation-Collection-with-Intervention)\n",
    "    1. [Activation Collection with Other Intervention](#Activation-Collection-at-Downstream-of-a-Intervened-Model)\n",
    "    1. [Intervene Single Neuron](#Intervene-on-a-Single-Neuron)\n",
    "    1. [Add New Intervention Type](#Add-New-Intervention-Type)\n",
    "    1. [Intervene on Recurrent NNs](#Recurrent-NNs-(Intervene-a-Specific-Timestep))\n",
    "    1. [Intervene across Times with RNNs](#Recurrent-NNs-(Intervene-cross-Time))\n",
    "    1. [Intervene on LM Generation](#LM-Generation)\n",
    "    1. [Saving and Loading](#Saving-and-Loading)\n",
    "    1. [Multi-Source Intervention (Parallel)](#Multi-Source-Interchange-Intervention-(Parallel-Mode))\n",
    "    1. [Multi-Source Intervention (Serial)](#Multi-Source-Interchange-Intervention-(Serial-Mode))\n",
    "    1. [Multi-Source Intervention with Subspaces (Parallel)](#Multi-Source-Interchange-Intervention-with-Subspaces-(Parallel-Mode))\n",
    "    1. [Multi-Source Intervention with Subspaces (Serial)](#Multi-Source-Interchange-Intervention-with-Subspaces-(Serial-Mode))\n",
    "    1. [Interchange Intervention Training](#Interchange-Intervention-Training-(IIT))\n",
    "1. [pyvene 102](#pyvene-102)\n",
    "    1. [Intervention Grouping](#Grouping)\n",
    "    1. [Intervention Skipping](#Intervention-Skipping-in-Runtime)\n",
    "    1. [Subspace Partition](#Subspace-Partition)\n",
    "    1. [Intervention Linking](#Intervention-Linking)\n",
    "    1. [Add New Model Type](#Add-New-Model-Type)\n",
    "    1. [Path Patching](#Composing-Complex-Intervention-Schema:-Path-Patching)\n",
    "    1. [Causal Tracing](#Composing-Complex-Intervention-Schema:-Causal-Tracing-in-15-lines)\n",
    "1. [The End](#The-End)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706e21b",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08304ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import pyvene\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    !pip install git+https://github.com/stanfordnlp/pyvene.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede4f94",
   "metadata": {},
   "source": [
    "## pyvene 101\n",
    "Before we get started, here are a couple of core notations that are used in this library:\n",
    "- **Base** example: this is the example we are intervening on, or, we are intervening on the computation graph of the model running the **Base** example.\n",
    "- **Source** example or representations: this is the source of our intervention. We use **Source** to intervene on **Base**.\n",
    "- **component**: this is the `nn.module` we are intervening in a pytorch-based NN.\n",
    "- **unit**: this is the axis of our intervention. If we say our **unit** is `pos` (`position`), then you are intervening on each token position.\n",
    "- **unit_locations**: this list gives you the percisely location of your intervention. It is the locations of the unit of analysis you are specifying. For instance, if your `unit` is `pos`, and your `unit_location` is 3, then it means you are intervening on the third token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245643b-fd44-47a5-a189-ce1565da7e25",
   "metadata": {},
   "source": [
    "### Get Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17c7f2f6-b0d3-4fe2-8e4f-c044b93f3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import pyvene as pv\n",
    "from circuitsvis.attention import attention_patterns\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"attention_weight\",\n",
    "    \"intervention_type\": pv.CollectIntervention}, model=gpt2)\n",
    "\n",
    "base = \"When John and Mary went to the shops, Mary gave the bag to\"\n",
    "collected_attn_w = pv_gpt2(\n",
    "    base = tokenizer(base, return_tensors=\"pt\"\n",
    "    ), unit_locations={\"base\": ([[[h for h in range(12)]]])}\n",
    ")[0][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7554177-d879-4e7a-968c-c16d1c1a169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-7ba1df49-5e8b\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-7ba1df49-5e8b\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"When\", \"\\u0120John\", \"\\u0120and\", \"\\u0120Mary\", \"\\u0120went\", \"\\u0120to\", \"\\u0120the\", \"\\u0120shops\", \",\", \"\\u0120Mary\", \"\\u0120gave\", \"\\u0120the\", \"\\u0120bag\", \"\\u0120to\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9241377115249634, 0.07586227357387543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8517740964889526, 0.12178844213485718, 0.026437407359480858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8089022040367126, 0.09462963044643402, 0.014816769398748875, 0.08165143430233002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9529829621315002, 0.006255049724131823, 0.0059762680903077126, 0.012639235705137253, 0.022146515548229218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7058243751525879, 0.07190630584955215, 0.011234955862164497, 0.15282848477363586, 0.031309425830841064, 0.026896387338638306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8924883604049683, 0.01721753552556038, 0.00922597199678421, 0.031797874718904495, 0.012842887081205845, 0.01896686851978302, 0.017460446804761887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9209375381469727, 0.03539547696709633, 0.0011913662310689688, 0.022587837651371956, 0.002276655752211809, 0.0005780707579106092, 0.0007757722632959485, 0.01625724881887436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.521770715713501, 0.14532102644443512, 0.009011348709464073, 0.27018287777900696, 0.006731100380420685, 0.002001181710511446, 0.0031778491102159023, 0.030911443755030632, 0.010892513208091259, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7849776744842529, 0.04032663255929947, 0.006243912503123283, 0.037558842450380325, 0.023119119927287102, 0.004092070274055004, 0.002674113493412733, 0.03330649808049202, 0.01405695267021656, 0.05364411324262619, 0.0, 0.0, 0.0, 0.0], [0.43821316957473755, 0.24088409543037415, 0.007644741330295801, 0.15145811438560486, 0.0041822209022939205, 0.0036045878659933805, 0.0033012102358043194, 0.024714982137084007, 0.008136066608130932, 0.11070460081100464, 0.007156202103942633, 0.0, 0.0, 0.0], [0.682963490486145, 0.040019892156124115, 0.012868871912360191, 0.037480492144823074, 0.006645917426794767, 0.007129838224500418, 0.0026869249995797873, 0.15977388620376587, 0.00980380829423666, 0.023398125544190407, 0.010183785110712051, 0.007044851779937744, 0.0, 0.0], [0.8686972260475159, 0.018655044957995415, 0.002267861505970359, 0.017491424456238747, 0.0031333521474152803, 0.0016992067685350776, 0.0016300231218338013, 0.020883845165371895, 0.011156802996993065, 0.024561727419495583, 0.0040056235156953335, 0.0035881020594388247, 0.022229691967368126, 0.0], [0.48664581775665283, 0.318038672208786, 0.009040072560310364, 0.1196301057934761, 0.00363082904368639, 0.0014455950586125255, 0.0010937826009467244, 0.009493325836956501, 0.005604012869298458, 0.03417867049574852, 0.0009108450030907989, 0.0019419373711571097, 0.0035329745151102543, 0.004813267849385738]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.987877368927002, 0.012122630141675472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9440780282020569, 0.04525846242904663, 0.01066349446773529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9493682980537415, 0.01303904503583908, 0.00937491562217474, 0.028217710554599762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9845219850540161, 0.0009183082147501409, 0.0025826545897871256, 0.0009274058393202722, 0.011049727909266949, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.960560142993927, 0.009927845560014248, 0.0005359183414839208, 0.02159503474831581, 0.0035579658579081297, 0.0038231112994253635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9819006323814392, 0.0016385553171858191, 0.0005504998844116926, 0.0030623851343989372, 0.0009610717534087598, 0.001503473031334579, 0.010383321903645992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9779987335205078, 0.0066282423213124275, 0.000492912600748241, 0.0020550466142594814, 0.0028791632503271103, 0.0017755933804437518, 0.0026781742926687002, 0.005492147523909807, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7413712739944458, 0.09052983671426773, 0.0040361834689974785, 0.14991533756256104, 0.002740828786045313, 0.0012191118439659476, 0.0007983013056218624, 0.003674708306789398, 0.005714421160519123, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9200074076652527, 0.010293026454746723, 0.019253883510828018, 0.017652908340096474, 0.009599541313946247, 0.002071698196232319, 0.001592810032889247, 0.003809242742136121, 0.0022518523037433624, 0.013467742130160332, 0.0, 0.0, 0.0, 0.0], [0.7378050088882446, 0.1011245995759964, 0.0016106859548017383, 0.05733047425746918, 0.0016006422229111195, 0.0009703110554255545, 0.0006028554635122418, 0.0013349198270589113, 0.0030085379257798195, 0.07858335971832275, 0.016028692945837975, 0.0, 0.0, 0.0], [0.8981754779815674, 0.014385129325091839, 0.003055012319236994, 0.014184700325131416, 0.0016813812544569373, 0.0010465907398611307, 0.0020278533920645714, 0.013254552148282528, 0.002302380045875907, 0.010710845701396465, 0.008955423720180988, 0.03022073023021221, 0.0, 0.0], [0.9775651693344116, 0.003968000877648592, 0.0002698129683267325, 0.0015020669670775533, 0.0003250486624892801, 0.0003282864054199308, 0.00035111868055537343, 0.0008062244160100818, 0.001691394834779203, 0.0034790567588061094, 0.0010048237163573503, 0.0024613358546048403, 0.006247684359550476, 0.0], [0.6649390459060669, 0.17555056512355804, 0.006488490849733353, 0.08572036772966385, 0.0015620585763826966, 0.00037238546065054834, 0.0006706378771923482, 0.0013264680746942759, 0.0017123647266998887, 0.050930049270391464, 0.0022520385682582855, 0.0031998888589441776, 0.0025660863611847162, 0.0027094718534499407]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9823833703994751, 0.017616624012589455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8383111357688904, 0.13831058144569397, 0.023378312587738037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9290463924407959, 0.030813025310635567, 0.024138344451785088, 0.016002139076590538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9724346995353699, 0.004806743934750557, 0.004492118488997221, 0.003060483606532216, 0.015205939300358295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8905816674232483, 0.057800378650426865, 0.001516709104180336, 0.038805413991212845, 0.00477208849042654, 0.006523816846311092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.966990053653717, 0.013807836920022964, 0.0014458410441875458, 0.006090929266065359, 0.002107083797454834, 0.0034759838599711657, 0.006082251202315092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8822508454322815, 0.07879015803337097, 0.005132833030074835, 0.010303378105163574, 0.008625871501863003, 0.008155323565006256, 0.0016600984381511807, 0.005081570707261562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5723040699958801, 0.23149210214614868, 0.00863628089427948, 0.1660338193178177, 0.003176081692799926, 0.0013585033593699336, 0.0018202568171545863, 0.0027606342919170856, 0.0124181704595685, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8201102614402771, 0.024712013080716133, 0.04069393873214722, 0.020094474777579308, 0.03758712112903595, 0.006062311120331287, 0.0014268554514274001, 0.006182681303471327, 0.025777442380785942, 0.017352845519781113, 0.0, 0.0, 0.0, 0.0], [0.4906851351261139, 0.16447657346725464, 0.013147196732461452, 0.1922224760055542, 0.006782026961445808, 0.0020767792593687773, 0.0015038845594972372, 0.0028062681667506695, 0.009208452887833118, 0.10168109834194183, 0.015409973450005054, 0.0, 0.0, 0.0], [0.7371206283569336, 0.04361879825592041, 0.008668778464198112, 0.09130841493606567, 0.009562015533447266, 0.0023770988918840885, 0.001579292002134025, 0.033478014171123505, 0.008962269872426987, 0.019684400409460068, 0.013179820962250233, 0.030460471287369728, 0.0, 0.0], [0.8600700497627258, 0.04299488663673401, 0.0032455993350595236, 0.02643294259905815, 0.007178249303251505, 0.002100930316373706, 0.0007776686106808484, 0.001488395850174129, 0.008091816678643227, 0.01757664792239666, 0.006544200237840414, 0.009515214711427689, 0.013983359560370445, 0.0], [0.48322543501853943, 0.21280643343925476, 0.030611416324973106, 0.18244677782058716, 0.004574385937303305, 0.0013432614505290985, 0.0010461865458637476, 0.001681879861280322, 0.00793082732707262, 0.04276004806160927, 0.00607064226642251, 0.01521975826472044, 0.0026135563384741545, 0.007669349666684866]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9541614651679993, 0.04583847522735596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8506830930709839, 0.12705886363983154, 0.022258058190345764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8853853344917297, 0.04603587090969086, 0.01291764248162508, 0.0556611642241478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9025852084159851, 0.0022348626516759396, 0.007552429102361202, 0.0021282967645674944, 0.08549918979406357, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8281477093696594, 0.02572396770119667, 0.015473361127078533, 0.02645258419215679, 0.03213335573673248, 0.07206910848617554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8684309720993042, 0.0069241574965417385, 0.016330815851688385, 0.013924616388976574, 0.03009023889899254, 0.03718888759613037, 0.027110207825899124, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.838818371295929, 0.020010437816381454, 0.011271211318671703, 0.01250483002513647, 0.028507564216852188, 0.026659173890948296, 0.020379792898893356, 0.04184863343834877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7929074764251709, 0.06044061854481697, 0.008223526179790497, 0.10573097318410873, 0.005331742577254772, 0.003477722406387329, 0.0044890278950333595, 0.0060667539946734905, 0.013332254253327847, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8284211158752441, 0.012508188374340534, 0.015298932790756226, 0.029461627826094627, 0.03622874617576599, 0.011493263766169548, 0.004726739600300789, 0.011322611942887306, 0.026363680139183998, 0.024175098165869713, 0.0, 0.0, 0.0, 0.0], [0.6386988162994385, 0.08175453543663025, 0.011347954161465168, 0.12663544714450836, 0.009052237495779991, 0.006605185102671385, 0.007445183582603931, 0.01172194629907608, 0.012434014119207859, 0.08724343031644821, 0.0070612612180411816, 0.0, 0.0, 0.0], [0.7172946929931641, 0.03222982585430145, 0.027208810672163963, 0.06802282482385635, 0.020709777250885963, 0.007312816102057695, 0.005290635861456394, 0.04350123554468155, 0.010912390425801277, 0.026192933320999146, 0.009746800176799297, 0.03157728165388107, 0.0, 0.0], [0.8825850486755371, 0.01203941646963358, 0.004827077966183424, 0.0070721800439059734, 0.005426874849945307, 0.005556480959057808, 0.0035117720253765583, 0.0075971889309585094, 0.023217175155878067, 0.010962342843413353, 0.005554623901844025, 0.02582276239991188, 0.005827064160257578, 0.0], [0.5738404989242554, 0.14683039486408234, 0.0192022155970335, 0.15919068455696106, 0.008885623887181282, 0.0017581358551979065, 0.003335318760946393, 0.0034766336902976036, 0.0037660449743270874, 0.060801099985837936, 0.0037436410784721375, 0.011178400367498398, 0.00044535123743116856, 0.003545949701219797]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9282837510108948, 0.07171626389026642, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6064638495445251, 0.32958295941352844, 0.06395316869020462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8863105177879333, 0.032462697476148605, 0.0631609559059143, 0.018065841868519783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9010186791419983, 0.01671689935028553, 0.012829759158194065, 0.01050330325961113, 0.05893133208155632, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6253466606140137, 0.09874667972326279, 0.017644967883825302, 0.05918536335229874, 0.12960241734981537, 0.06947386264801025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7473354935646057, 0.03818093612790108, 0.019209468737244606, 0.028027918189764023, 0.06946966797113419, 0.05972742661833763, 0.03804900869727135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8029201030731201, 0.017443377524614334, 0.01496036909520626, 0.007839158177375793, 0.01920895278453827, 0.042330771684646606, 0.05842955410480499, 0.036867767572402954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6853805184364319, 0.08337407559156418, 0.018005840480327606, 0.05717959627509117, 0.035298578441143036, 0.019145844504237175, 0.011633474379777908, 0.04177853465080261, 0.048203565180301666, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4552716016769409, 0.018448568880558014, 0.060341976583004, 0.009874869138002396, 0.08924292027950287, 0.03176352009177208, 0.010657858103513718, 0.051721323281526566, 0.2143440991640091, 0.05833318457007408, 0.0, 0.0, 0.0, 0.0], [0.6891724467277527, 0.05755690112709999, 0.032548703253269196, 0.018793907016515732, 0.02452031709253788, 0.020200494676828384, 0.019958103075623512, 0.020146682858467102, 0.04323696345090866, 0.03500862792134285, 0.038856882601976395, 0.0, 0.0, 0.0], [0.5739867687225342, 0.012777949683368206, 0.03158310800790787, 0.00836885441094637, 0.039591312408447266, 0.03300393745303154, 0.015603398904204369, 0.08228663355112076, 0.027150345966219902, 0.014029121026396751, 0.1267319768667221, 0.034886617213487625, 0.0, 0.0], [0.7308521270751953, 0.02396642044186592, 0.008650668896734715, 0.007442462723702192, 0.011741062626242638, 0.01453279610723257, 0.022113928571343422, 0.021365458145737648, 0.047714900225400925, 0.02109246701002121, 0.03532763943076134, 0.03730570897459984, 0.017894478514790535, 0.0], [0.573961079120636, 0.18412795662879944, 0.026032596826553345, 0.037396617233753204, 0.01844066195189953, 0.00749259814620018, 0.012986707501113415, 0.009839033707976341, 0.012842139229178429, 0.05099373310804367, 0.015041780658066273, 0.0201250072568655, 0.00756830582395196, 0.023151706904172897]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9870705008506775, 0.012929506599903107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9490176439285278, 0.040326979011297226, 0.010655463673174381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.695199191570282, 0.075804203748703, 0.15539108216762543, 0.07360544800758362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9086331129074097, 0.014510341919958591, 0.016260748729109764, 0.01930893026292324, 0.041286833584308624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9585595726966858, 0.020961597561836243, 0.0035761171020567417, 0.007964594289660454, 0.006057849153876305, 0.0028802044689655304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9709068536758423, 0.012767442502081394, 0.002551752608269453, 0.002787994220852852, 0.004802490584552288, 0.004855392500758171, 0.001328150276094675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3606973886489868, 0.04616302251815796, 0.014035677537322044, 0.023484207689762115, 0.24733243882656097, 0.13311439752578735, 0.08295973390340805, 0.09221317619085312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.725151538848877, 0.029176410287618637, 0.005250656045973301, 0.02790153957903385, 0.04527896270155907, 0.02365717478096485, 0.015084540471434593, 0.08383092284202576, 0.044668346643447876, 0.0, 0.0, 0.0, 0.0, 0.0], [0.232200488448143, 0.0057655512355268, 0.015339878387749195, 0.005193775985389948, 0.01353427954018116, 0.012530025094747543, 0.011596553958952427, 0.030101273208856583, 0.6098286509513855, 0.06390946358442307, 0.0, 0.0, 0.0, 0.0], [0.7828466892242432, 0.01013961248099804, 0.0051816729828715324, 0.0064100646413862705, 0.0076995668932795525, 0.0066917333751916885, 0.018549839034676552, 0.014979206956923008, 0.07338622957468033, 0.03332585468888283, 0.04078947380185127, 0.0, 0.0, 0.0], [0.9521311521530151, 0.00434495834633708, 0.0018577274167910218, 0.0008987054461613297, 0.0021993753034621477, 0.0025815307162702084, 0.0011365263490006328, 0.003403875743970275, 0.009551992639899254, 0.004926531109958887, 0.0129573754966259, 0.004010303877294064, 0.0, 0.0], [0.7921712398529053, 0.005263972096145153, 0.003790148301050067, 0.001487864414229989, 0.006084544584155083, 0.005326799117028713, 0.016542579978704453, 0.002728006336838007, 0.011142202652990818, 0.006650296971201897, 0.06792741268873215, 0.06644108146429062, 0.014443821273744106, 0.0], [0.7339237928390503, 0.01729143038392067, 0.002743955934420228, 0.0051257978193461895, 0.006512291729450226, 0.005571636836975813, 0.014128164388239384, 0.0060926987789571285, 0.029509050771594048, 0.021437209099531174, 0.037400342524051666, 0.06171126291155815, 0.0423380583524704, 0.01621430553495884]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9022049903869629, 0.09779498726129532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.864239513874054, 0.127393901348114, 0.008366556838154793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8985954523086548, 0.07338915020227432, 0.010898460634052753, 0.017116844654083252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9793285131454468, 0.009315711446106434, 0.0030482185538858175, 0.003568159881979227, 0.004739437252283096, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8951767086982727, 0.06309178471565247, 0.002806453499943018, 0.020234664902091026, 0.007925835438072681, 0.010764501988887787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9483073353767395, 0.0215194933116436, 0.002851644763723016, 0.006091877352446318, 0.004681907128542662, 0.006674567703157663, 0.009873194620013237, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9411106705665588, 0.03698405995965004, 0.0005124953459016979, 0.0029620195273309946, 0.002070570830255747, 0.001939876819960773, 0.0012351344339549541, 0.013185080140829086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5845963358879089, 0.2829763889312744, 0.0034650457091629505, 0.09221882373094559, 0.00251192064024508, 0.0019069323316216469, 0.002087026135995984, 0.023052893579006195, 0.007184556685388088, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8741837739944458, 0.04073411598801613, 0.00612194649875164, 0.012019653804600239, 0.015965977683663368, 0.002758932765573263, 0.0023271094541996717, 0.026604965329170227, 0.005720232613384724, 0.01356325950473547, 0.0, 0.0, 0.0, 0.0], [0.47131606936454773, 0.41869717836380005, 0.0015128145460039377, 0.04598969966173172, 0.0023681658785790205, 0.0007737193373031914, 0.0014666105853393674, 0.01638445444405079, 0.0030448317993432283, 0.03325333446264267, 0.005193180870264769, 0.0, 0.0, 0.0], [0.6481140851974487, 0.11395079642534256, 0.012576221488416195, 0.014144793152809143, 0.005393137224018574, 0.006935139652341604, 0.005663743708282709, 0.1107797846198082, 0.013822727836668491, 0.009440572001039982, 0.019992517307400703, 0.0391865037381649, 0.0, 0.0], [0.9205898642539978, 0.013825338333845139, 0.0004725826147478074, 0.0019193199696019292, 0.0017628440400585532, 0.0010729329660534859, 0.0003166171081829816, 0.007865820080041885, 0.007066880352795124, 0.002827327698469162, 0.004449469968676567, 0.001447442569769919, 0.03638358414173126, 0.0], [0.3619619905948639, 0.588272213935852, 0.0018372989725321531, 0.021676160395145416, 0.0010959265055134892, 0.0003592674620449543, 0.0006212761509232223, 0.008643001317977905, 0.0014201926533132792, 0.00788390077650547, 0.0006979182362556458, 0.001216919394209981, 0.003441720735281706, 0.000872155767865479]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9244219660758972, 0.0755779892206192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7250919938087463, 0.22935348749160767, 0.045554496347904205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8363485932350159, 0.047610051929950714, 0.05896671488881111, 0.05707471817731857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9307315349578857, 0.0027512656524777412, 0.009578102268278599, 0.001475987839512527, 0.05546307563781738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6907782554626465, 0.05329706519842148, 0.039799194782972336, 0.0804731622338295, 0.05061193183064461, 0.08504042029380798, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7943143248558044, 0.01708281971514225, 0.03544310852885246, 0.027524011209607124, 0.009694209322333336, 0.07523881644010544, 0.04070273041725159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8161590695381165, 0.020156878978013992, 0.029733162373304367, 0.0054382276721298695, 0.019527114927768707, 0.02714713104069233, 0.013363662175834179, 0.0684746578335762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28511783480644226, 0.324409157037735, 0.059258051216602325, 0.23564447462558746, 0.011132570914924145, 0.013237646780908108, 0.01141025684773922, 0.011546185240149498, 0.048243891447782516, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7301113605499268, 0.0258678887039423, 0.05633596330881119, 0.009573073126375675, 0.06743884086608887, 0.022730931639671326, 0.008261682465672493, 0.006087929476052523, 0.05550229549407959, 0.018090037629008293, 0.0, 0.0, 0.0, 0.0], [0.26660847663879395, 0.2851044535636902, 0.027388937771320343, 0.1310860514640808, 0.011721363291144371, 0.012505428865551949, 0.009511519223451614, 0.002298290142789483, 0.03640905022621155, 0.14215104281902313, 0.07521544396877289, 0.0, 0.0, 0.0], [0.4543800950050354, 0.06635568290948868, 0.05650682747364044, 0.03685707971453667, 0.0089968116953969, 0.026931386440992355, 0.02746833674609661, 0.11066506803035736, 0.04157489910721779, 0.025626378133893013, 0.04159531742334366, 0.1030421108007431, 0.0, 0.0], [0.8159105181694031, 0.013844111934304237, 0.007573550567030907, 0.008808636106550694, 0.0035531013272702694, 0.004716229625046253, 0.002277073683217168, 0.010624284856021404, 0.017036806792020798, 0.01727551780641079, 0.018920322880148888, 0.008328725583851337, 0.0711311548948288, 0.0], [0.05987198278307915, 0.6699920296669006, 0.025970669463276863, 0.1397266387939453, 0.003968910779803991, 0.002555175917223096, 0.003925333730876446, 0.00020000470976810902, 0.007123017683625221, 0.05973844975233078, 0.002907689195126295, 0.01511324755847454, 0.00015996486763469875, 0.008746810257434845]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9854966998100281, 0.014503341168165207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697378873825073, 0.012374036014080048, 0.017888111993670464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9659683108329773, 0.006601185537874699, 0.01504522655159235, 0.012385288253426552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9450362920761108, 0.001237727701663971, 0.0028388125356286764, 0.0005801038350909948, 0.05030703917145729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829158782958984, 0.003514775773510337, 0.0024619598407298326, 0.002820880152285099, 0.0020476370118558407, 0.006238971836864948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9837080836296082, 0.001707560964860022, 0.004349163267761469, 0.001778756151907146, 0.0016272533684968948, 0.0034916193690150976, 0.003337625879794359, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9663838744163513, 0.003290557535365224, 0.0038803184870630503, 0.0013615126954391599, 0.002119863172993064, 0.009311845526099205, 0.0038815063890069723, 0.009770574979484081, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9533470273017883, 0.00807543657720089, 0.006827176082879305, 0.0058053769171237946, 0.0014199254801496863, 0.0069619594141840935, 0.006178613286465406, 0.0019997935742139816, 0.009384861215949059, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9215651750564575, 0.0061365412548184395, 0.03696824610233307, 0.004865605849772692, 0.003560099983587861, 0.009388209320604801, 0.0021529714576900005, 0.0019007236696779728, 0.008981890045106411, 0.004480579402297735, 0.0, 0.0, 0.0, 0.0], [0.9538441300392151, 0.010714951902627945, 0.005282208323478699, 0.0033547913189977407, 0.0012650934513658285, 0.0027047444600611925, 0.0034849229268729687, 0.0018060844158753753, 0.005480195861309767, 0.002142493613064289, 0.009920282289385796, 0.0, 0.0, 0.0], [0.9198287129402161, 0.005531433504074812, 0.010177400894463062, 0.004157776013016701, 0.0026330696418881416, 0.0031407326459884644, 0.000777150911744684, 0.030665365979075432, 0.0036719164345413446, 0.0016491642454639077, 0.009127097204327583, 0.008640125393867493, 0.0, 0.0], [0.9589753150939941, 0.0014420340303331614, 0.004346976988017559, 0.000356927776010707, 0.00102028320543468, 0.0032798827160149813, 0.0012603237992152572, 0.0014217686839401722, 0.008908829651772976, 0.0002691452391445637, 0.00168622937053442, 0.014716519974172115, 0.002315630903467536, 0.0], [0.8911623954772949, 0.010473505593836308, 0.011356648057699203, 0.005754950921982527, 0.0016657378291711211, 0.0020177303813397884, 0.011167280375957489, 0.0016458628233522177, 0.005858095828443766, 0.002109740162268281, 0.004861842840909958, 0.04102187976241112, 0.0014734409051015973, 0.009430911391973495]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9918562173843384, 0.008143814280629158, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8644002079963684, 0.07359600067138672, 0.0620037242770195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8224768042564392, 0.09337860345840454, 0.05286039784550667, 0.03128417953848839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8369284272193909, 0.03058815747499466, 0.0040184264071285725, 0.05146608501672745, 0.07699888199567795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7115703821182251, 0.04570873826742172, 0.009649206884205341, 0.09798499196767807, 0.09399593621492386, 0.041090670973062515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8718491196632385, 0.010232232511043549, 0.010146545246243477, 0.033385902643203735, 0.017669769003987312, 0.04734496772289276, 0.009371467866003513, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2396210879087448, 0.03030978888273239, 0.010836154222488403, 0.031330954283475876, 0.3038201630115509, 0.1623212695121765, 0.13378757238388062, 0.0879729688167572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2847372889518738, 0.055503856390714645, 0.008553733117878437, 0.10795752704143524, 0.21144936978816986, 0.12372501939535141, 0.01770767942070961, 0.043634116649627686, 0.14673146605491638, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7945039868354797, 0.030747998505830765, 0.0035688835196197033, 0.012281038798391819, 0.010056591592729092, 0.007251765113323927, 0.0043604448437690735, 0.005149067845195532, 0.017209067940711975, 0.11487128585577011, 0.0, 0.0, 0.0, 0.0], [0.4259013235569, 0.09206041693687439, 0.006831950508058071, 0.04028337076306343, 0.04137555509805679, 0.048426639288663864, 0.011490683071315289, 0.053887989372015, 0.1838158369064331, 0.059246428310871124, 0.03667976334691048, 0.0, 0.0, 0.0], [0.7983818650245667, 0.011203387752175331, 0.00560775538906455, 0.020031943917274475, 0.01320268027484417, 0.014726947993040085, 0.004066138062626123, 0.008213075809180737, 0.028555283322930336, 0.019343269988894463, 0.060649026185274124, 0.016018567606806755, 0.0, 0.0], [0.430984228849411, 0.007851975969970226, 0.0033983062021434307, 0.0029521165415644646, 0.0314783938229084, 0.01737299934029579, 0.03658834472298622, 0.003299230709671974, 0.004903062246739864, 0.015599395148456097, 0.10359319299459457, 0.2268681526184082, 0.11511065065860748, 0.0], [0.3156508803367615, 0.04180462285876274, 0.014400233514606953, 0.03817130997776985, 0.02349221706390381, 0.03885722532868385, 0.01581229455769062, 0.02958257868885994, 0.17306730151176453, 0.06207278370857239, 0.08051391690969467, 0.04711225628852844, 0.05331355705857277, 0.06614881008863449]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9860289692878723, 0.01397103350609541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9536663889884949, 0.029262764379382133, 0.01707088202238083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502556324005127, 0.012740746140480042, 0.01691499352455139, 0.020088614895939827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9764211773872375, 0.0015524046029895544, 0.004284904338419437, 0.005622622091323137, 0.012118912301957607, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9348918795585632, 0.011569766327738762, 0.0022925252560526133, 0.042663998901844025, 0.002843432826921344, 0.00573842553421855, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9661884903907776, 0.00268564373254776, 0.002327632624655962, 0.006852383259683847, 0.002523066010326147, 0.007228126749396324, 0.012194515205919743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97343510389328, 0.010960396379232407, 0.0007667901809327304, 0.004013884346932173, 0.002435519127175212, 0.0017835980979725718, 0.0023698366712778807, 0.004234880208969116, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7099213600158691, 0.11195787042379379, 0.007633039262145758, 0.15312400460243225, 0.0024489809293299913, 0.001496042124927044, 0.0026682058814913034, 0.0036132135428488255, 0.007137279491871595, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9255748391151428, 0.008676350116729736, 0.016341205686330795, 0.014992974698543549, 0.005851410795003176, 0.002077462151646614, 0.0018906228942796588, 0.0034096185117959976, 0.006567603442817926, 0.01461795810610056, 0.0, 0.0, 0.0, 0.0], [0.5549242496490479, 0.17279912531375885, 0.00847515556961298, 0.1537933498620987, 0.0014268705854192376, 0.0018837307579815388, 0.0033755383919924498, 0.003810953116044402, 0.009313278831541538, 0.08466304838657379, 0.0055346181616187096, 0.0, 0.0, 0.0], [0.8349324464797974, 0.020945101976394653, 0.005566143896430731, 0.03525768965482712, 0.0029989576432853937, 0.0034695500507950783, 0.0024289207067340612, 0.043226804584264755, 0.010508880019187927, 0.014472239650785923, 0.0052979448810219765, 0.020895294845104218, 0.0, 0.0], [0.9064263701438904, 0.017752543091773987, 0.0018515808042138815, 0.012698283419013023, 0.0014612538507208228, 0.0024709992576390505, 0.0022909753024578094, 0.0029612707439810038, 0.010147345252335072, 0.0165229681879282, 0.0019642619881778955, 0.011083822697401047, 0.012368264608085155, 0.0], [0.7680356502532959, 0.13279414176940918, 0.010307551361620426, 0.04663461074233055, 0.0013755419058725238, 0.0010623617563396692, 0.0019285574089735746, 0.001338146859779954, 0.0052393339574337006, 0.018516123294830322, 0.001051678555086255, 0.0052118971943855286, 0.0019113561138510704, 0.004593135789036751]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732680320739746, 0.02673191763460636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9137605428695679, 0.06745292246341705, 0.018786534667015076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9350345134735107, 0.04351172223687172, 0.01681475341320038, 0.004638982005417347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9883273243904114, 0.004664964973926544, 0.00174737349152565, 0.002279898151755333, 0.002980401273816824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732941389083862, 0.0025268159806728363, 0.00038101241807453334, 0.000553733843844384, 0.0010656763333827257, 0.02217857725918293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42627885937690735, 0.0006563509232364595, 0.00046434602700173855, 9.46360596572049e-05, 0.0005391632439568639, 0.5264629125595093, 0.04550384357571602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.957434892654419, 0.0053536477498710155, 0.0005160701693966985, 0.0003672557941172272, 0.004688273649662733, 0.008860306814312935, 0.015887951478362083, 0.006891473196446896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9032186269760132, 0.037643153220415115, 0.006694566924124956, 0.019459860399365425, 0.005272358190268278, 0.004146787337958813, 0.0010773250833153725, 0.006157943978905678, 0.01632930524647236, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8130955100059509, 0.0030696981120854616, 0.022284213453531265, 0.0022431525867432356, 0.0030852912459522486, 0.001330738770775497, 0.00031069506076164544, 0.003373807994648814, 0.12610355019569397, 0.025103282183408737, 0.0, 0.0, 0.0, 0.0], [0.807578980922699, 0.02403074875473976, 0.000930976530071348, 0.008222231641411781, 0.0022441435139626265, 0.0006313375779427588, 0.0001396071311319247, 0.0028652602341026068, 0.019678210839629173, 0.025839492678642273, 0.10783902555704117, 0.0, 0.0, 0.0], [0.39846453070640564, 0.0033721979707479477, 0.0017138064140453935, 0.0014807116240262985, 0.0032869090791791677, 0.0009841275168582797, 6.975438736844808e-05, 0.0024719724897295237, 0.01355122309178114, 0.0024804940912872553, 0.5158988237380981, 0.0562254823744297, 0.0, 0.0], [0.8912891149520874, 0.0019343760795891285, 0.0003569350519683212, 0.000420279597165063, 0.00314726703800261, 0.002107321284711361, 0.0028179893270134926, 0.002287977607920766, 0.0032614765223115683, 0.0014584034215658903, 0.04837338253855705, 0.03287475183606148, 0.009670713916420937, 0.0], [0.8187775015830994, 0.05249674618244171, 0.006832740735262632, 0.03383294865489006, 0.0008208426297642291, 0.0008685264037922025, 0.000820092624053359, 0.0014523561112582684, 0.0023442311212420464, 0.04766827076673508, 0.008511531166732311, 0.002579974941909313, 0.006042378023266792, 0.016951769590377808]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f47d7ecf4c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_patterns(tokens=tokenizer.tokenize(base), attention=collected_attn_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5addb-4bd7-4129-b350-0677774f5790",
   "metadata": {},
   "source": [
    "### Set Activation to Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82664f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "# define the component to zero-out\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "    \"layer\": 0, \"component\": \"mlp_output\",\n",
    "    \"source_representation\": torch.zeros(gpt2.config.n_embd)\n",
    "}, model=gpt2)\n",
    "# run the intervened forward pass\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"), \n",
    "    # we define the intervening token dynamically\n",
    "    unit_locations={\"base\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39071858",
   "metadata": {},
   "source": [
    "### Set Activations to Zeros with Subspaces\n",
    "The notion of subspace means the actual dimensions you are intervening. If we have a representation in a size of 512, the first 128 activation values are its subspace activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7896c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "Directory './tmp/' already exists.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "# built-in helper to get a HuggingFace model\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "# create with dict-based config\n",
    "pv_config = pv.IntervenableConfig({\n",
    "  \"layer\": 0, \"component\": \"mlp_output\"})\n",
    "#initialize model\n",
    "pv_gpt2 = pv.IntervenableModel(pv_config, model=gpt2)\n",
    "# run an intervened forward pass\n",
    "intervened_outputs = pv_gpt2(\n",
    "  # the intervening base input\n",
    "  base=tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"), \n",
    "  # the location to intervene at (3rd token)\n",
    "  unit_locations={\"base\": 3},\n",
    "  # the individual dimensions targetted\n",
    "  subspaces=[10,11,12],\n",
    "  source_representations=torch.zeros(gpt2.config.n_embd)\n",
    ")\n",
    "# sharing\n",
    "pv_gpt2.save(\"./tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410904d",
   "metadata": {},
   "source": [
    "### Interchange Interventions\n",
    "Instead of a static vector, we can intervene the model with activations sampled from a different forward run. We call this interchange intervention, where intervention happens between two examples and we are interchanging activations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9691c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "# built-in helper to get a HuggingFace model\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "# create with dict-based config\n",
    "pv_config = pv.IntervenableConfig({\n",
    "  \"layer\": 0,\n",
    "  \"component\": \"mlp_output\"},\n",
    "  intervention_types=pv.VanillaIntervention\n",
    ")\n",
    "#initialize model\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "  pv_config, model=gpt2)\n",
    "# run an interchange intervention \n",
    "intervened_outputs = pv_gpt2(\n",
    "  # the base input\n",
    "  base=tokenizer(\n",
    "    \"The capital of Spain is\", \n",
    "    return_tensors = \"pt\"), \n",
    "  # the source input\n",
    "  sources=tokenizer(\n",
    "    \"The capital of Italy is\", \n",
    "    return_tensors = \"pt\"), \n",
    "  # the location to intervene at (3rd token)\n",
    "  unit_locations={\"sources->base\": 3},\n",
    "  # the individual dimensions targeted\n",
    "  subspaces=[10,11,12]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890fda4",
   "metadata": {},
   "source": [
    "### Intervention Configuration\n",
    "You can also initialize the config without the lazy dictionary passing by enabling more options, e.g., the mode of these interventions are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4faa3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "IntervenableConfig\n",
      "{\n",
      "    \"model_type\": \"None\",\n",
      "    \"representations\": [\n",
      "        {\n",
      "            \"layer\": 0,\n",
      "            \"component\": \"mlp_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": null,\n",
      "            \"intervention_type\": null,\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": \"PLACEHOLDER\",\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 1,\n",
      "            \"component\": \"mlp_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": null,\n",
      "            \"intervention_type\": null,\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": \"PLACEHOLDER\",\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 2,\n",
      "            \"component\": \"mlp_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": null,\n",
      "            \"intervention_type\": null,\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": \"PLACEHOLDER\",\n",
      "            \"hidden_source_representation\": null\n",
      "        },\n",
      "        {\n",
      "            \"layer\": 3,\n",
      "            \"component\": \"mlp_output\",\n",
      "            \"unit\": \"pos\",\n",
      "            \"max_number_of_units\": 1,\n",
      "            \"low_rank_dimension\": null,\n",
      "            \"intervention_type\": null,\n",
      "            \"subspace_partition\": null,\n",
      "            \"group_key\": null,\n",
      "            \"intervention_link_key\": null,\n",
      "            \"moe_key\": null,\n",
      "            \"source_representation\": \"PLACEHOLDER\",\n",
      "            \"hidden_source_representation\": null\n",
      "        }\n",
      "    ],\n",
      "    \"intervention_types\": \"<class 'pyvene.models.interventions.VanillaIntervention'>\",\n",
      "    \"mode\": \"parallel\",\n",
      "    \"interventions\": [\n",
      "        \"None\"\n",
      "    ],\n",
      "    \"sorted_keys\": \"None\",\n",
      "    \"intervention_dimensions\": \"None\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "# standalone configuration object\n",
    "config = pv.IntervenableConfig([\n",
    "    {\n",
    "        \"layer\": _,\n",
    "        \"component\": \"mlp_output\",\n",
    "        \"source_representation\": torch.zeros(\n",
    "            gpt2.config.n_embd)\n",
    "    } for _ in range(4)],\n",
    "    mode=\"parallel\"\n",
    ")\n",
    "# this object is serializable\n",
    "print(config)\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"), \n",
    "    unit_locations={\"base\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b2270",
   "metadata": {},
   "source": [
    "### Addition Intervention\n",
    "Activation swap is one kind of interventions we can perform. Here is another simple one: `pv.AdditionIntervention`, which adds the sampled representation into the **Base** run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40f5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 0,\n",
    "    \"component\": \"mlp_input\"},\n",
    "    pv.AdditionIntervention\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The Space Needle is in downtown\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    unit_locations={\"base\": [[[0, 1, 2, 3]]]},\n",
    "    source_representations = torch.rand(gpt2.config.n_embd)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ddf77",
   "metadata": {},
   "source": [
    "### Trainable Intervention\n",
    "Interventions can contain trainable parameters, and hook-up with the model to receive gradients end-to-end. They are often useful in searching for an particular interpretation of the representation.\n",
    "\n",
    "The following example does a single step gradient calculation to push the model to generate `Rome` after the intervention. If we can train such intervention at scale with low loss, it means you have a causal grab onto your model. In terms of interpretability, that means, somehow you find a representation (not the original one since its trained) that maps onto the `capital` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f058ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "das_config = pv.IntervenableConfig({\n",
    "    \"layer\": 8,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},\n",
    "    # this is a trainable low-rank rotation\n",
    "    pv.LowRankRotatedSpaceIntervention\n",
    ")\n",
    "\n",
    "das_gpt2 = pv.IntervenableModel(das_config, model=gpt2)\n",
    "\n",
    "last_hidden_state = das_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    sources = tokenizer(\n",
    "        \"The capital of Italy is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    unit_locations={\"sources->base\": 3}\n",
    ")[-1].last_hidden_state[:,-1]\n",
    "\n",
    "# golden counterfacutual label as Rome\n",
    "label = tokenizer.encode(\n",
    "    \" Rome\", return_tensors=\"pt\")\n",
    "logits = torch.matmul(\n",
    "    last_hidden_state, gpt2.wte.weight.t())\n",
    "\n",
    "m = torch.nn.CrossEntropyLoss()\n",
    "loss = m(logits, label.view(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd2b8e",
   "metadata": {},
   "source": [
    "### Activation Collection with Intervention\n",
    "You can also collect activations with our provided `pv.CollectIntervention` intervention. More importantly, this can be used interchangably with other interventions. You can collect something from an intervened model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6bd585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"block_output\",\n",
    "    \"intervention_type\": pv.CollectIntervention}\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    config, model=gpt2)\n",
    "\n",
    "collected_activations = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), unit_locations={\"sources->base\": 3}\n",
    ")[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0d0c6",
   "metadata": {},
   "source": [
    "### Activation Collection at Downstream of a Intervened Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adcfcb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 8,\n",
    "    \"component\": \"block_output\",\n",
    "    \"intervention_type\": pv.VanillaIntervention}\n",
    ")\n",
    "\n",
    "config.add_intervention({\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"block_output\",\n",
    "    \"intervention_type\": pv.CollectIntervention})\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    config, model=gpt2)\n",
    "\n",
    "collected_activations = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    sources = [tokenizer(\n",
    "        \"The capital of Italy is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), None], unit_locations={\"sources->base\": 3}\n",
    ")[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6e4d9",
   "metadata": {},
   "source": [
    "### Intervene on a Single Neuron\n",
    "We want to provide a good user interface so that interventions can be done easily by people with less pytorch or programming experience. Meanwhile, we also want to be flexible and provide the depth of control required for highly specific tasks. Here is an example where we intervene on a specific neuron at a specific head of a layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25b6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 8,\n",
    "    \"component\": \"head_attention_value_output\",\n",
    "    \"unit\": \"h.pos\",\n",
    "    \"intervention_type\": pv.CollectIntervention}\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    config, model=gpt2)\n",
    "\n",
    "collected_activations = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    unit_locations={\n",
    "        # GET_LOC is a helper.\n",
    "        # (3,3) means head 3 position 3\n",
    "        \"base\": pv.GET_LOC((3,3))\n",
    "    },\n",
    "    # the notion of subspace is used to target neuron 0.\n",
    "    subspaces=[0]\n",
    ")[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692bc15",
   "metadata": {},
   "source": [
    "### Add New Intervention Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1597221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "class MultiplierIntervention(\n",
    "  pv.ConstantSourceIntervention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    def forward(\n",
    "    self, base, source=None, subspaces=None):\n",
    "        return base * 99.0\n",
    "# run with new intervention type\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "  \"intervention_type\": MultiplierIntervention}, \n",
    "  model=gpt2)\n",
    "intervened_outputs = pv_gpt2(\n",
    "  base = tokenizer(\"The capital of Spain is\", \n",
    "    return_tensors=\"pt\"), \n",
    "  unit_locations={\"base\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079050f6",
   "metadata": {},
   "source": [
    "### Recurrent NNs (Intervene a Specific Timestep)\n",
    "Existing intervention libraries focus on Transformer models. They often lack of supports for GRUs, LSTMs or any state-space model. The fundemental problem is in the hook mechanism provided by PyTorch. Hook is attached to a module before runtime. Models like GRUs will lead to undesired callback from the hook as there is no notion of state or time of the hook. \n",
    "\n",
    "We make our hook stateful, so you can intervene on recurrent NNs like GRUs. This notion of time will become useful when intervening on Transformers yet want to unroll the causal effect during generation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a53347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, _, gru = pv.create_gru_classifier(\n",
    "    pv.GRUConfig(h_dim=32))\n",
    "\n",
    "pv_gru = pv.IntervenableModel({\n",
    "    \"component\": \"cell_output\",\n",
    "    \"unit\": \"t\", \n",
    "    \"intervention_type\": pv.ZeroIntervention},\n",
    "    model=gru)\n",
    "\n",
    "rand_t = torch.rand(1,10, gru.config.h_dim)\n",
    "\n",
    "intervened_outputs = pv_gru(\n",
    "  base = {\"inputs_embeds\": rand_t}, \n",
    "  unit_locations={\"base\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031dd5de",
   "metadata": {},
   "source": [
    "### Recurrent NNs (Intervene cross Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b48166c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "# built-in helper to get a GRU\n",
    "_, _, gru = pv.create_gru_classifier(\n",
    "    pv.GRUConfig(h_dim=32))\n",
    "# wrap it with config\n",
    "pv_gru = pv.IntervenableModel({\n",
    "    \"component\": \"cell_output\",\n",
    "    # intervening on time\n",
    "    \"unit\": \"t\", \n",
    "    \"intervention_type\": pv.ZeroIntervention},\n",
    "    model=gru)\n",
    "# run an intervened forward pass\n",
    "rand_b = torch.rand(1,10, gru.config.h_dim)\n",
    "rand_s = torch.rand(1,10, gru.config.h_dim)\n",
    "intervened_outputs = pv_gru(\n",
    "  base = {\"inputs_embeds\": rand_b}, \n",
    "  sources = [{\"inputs_embeds\": rand_s}], \n",
    "  # intervening time step\n",
    "  unit_locations={\"sources->base\": (6, 3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121366c1",
   "metadata": {},
   "source": [
    "### LMs Generation\n",
    "You can also intervene the generation call of LMs. Here is a simple example where we try to add a vector into the MLP output when the model decodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f718e2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was walking in the park when she saw a big, red balloon. She was so excited and wanted to play with it.\n",
      "\n",
      "But then, a big, mean man came and said, \"That balloon is mine! You can't have it!\" Lucy was very sad and started to cry.\n",
      "\n",
      "The man said, \"I'm sorry, but I need the balloon for my work. You can have it if you want.\"\n",
      "\n",
      "Lucy was so happy and said, \"Yes please!\" She took the balloon and ran away.\n",
      "\n",
      "But then, the man said, \"Wait! I have an idea. Let's make a deal. If you can guess what I'm going to give you, then you can have the balloon.\"\n",
      "\n",
      "Lucy thought for a moment and then said, \"I guess I'll have to get the balloon.\"\n",
      "\n",
      "The man smiled and said, \"That's a good guess! Here you go.\"\n",
      "\n",
      "Lucy was so happy and thanked the man. She hugged the balloon and ran off to show her mom.\n",
      "\n",
      "The end.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "# built-in helper to get tinystore\n",
    "_, tokenizer, tinystory = pv.create_gpt_neo()\n",
    "emb_happy = tinystory.transformer.wte(\n",
    "    torch.tensor(14628)) \n",
    "\n",
    "pv_tinystory = pv.IntervenableModel([{\n",
    "    \"layer\": l,\n",
    "    \"component\": \"mlp_output\",\n",
    "    \"intervention_type\": pv.AdditionIntervention\n",
    "    } for l in range(tinystory.config.num_layers)],\n",
    "    model=tinystory\n",
    ")\n",
    "# prompt and generate\n",
    "prompt = tokenizer(\n",
    "    \"Once upon a time there was\", return_tensors=\"pt\")\n",
    "_, intervened_story = pv_tinystory.generate(\n",
    "    tokenizer(\"Once upon a time there was\", return_tensors=\"pt\"),\n",
    "    source_representations=emb_happy*0.3, max_length=256\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(\n",
    "    intervened_story[0], \n",
    "    skip_special_tokens=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb539f4b",
   "metadata": {},
   "source": [
    "### Saving and Loading\n",
    "This is one of the benefits of program abstraction. We abstract out the intervention and its schema, so we have a user friendly interface. Furthermore, it allows us to have a serializable configuration file that tells everything about your configuration.\n",
    "\n",
    "You can then save, share and load interventions easily. Note that you still need your access to the data, if you need to sample **Source** representations from other examples. But we think this is doable via a separate HuggingFace datasets upload. In the future, there could be an option of coupling this configuration with a specific remote dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272f3773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "Directory './tmp/' already exists.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "# run with new intervention type\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "  \"intervention_type\": pv.ZeroIntervention}, \n",
    "  model=gpt2)\n",
    "\n",
    "pv_gpt2.save(\"./tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b894b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n",
      "WARNING:root:Loading trainable intervention from intkey_layer.0.repr.block_output.unit.pos.nunit.1#0.bin.\n"
     ]
    }
   ],
   "source": [
    "pv_gpt2 = pv.IntervenableModel.load(\n",
    "    \"./tmp/\",\n",
    "    model=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d07ca8",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention (Parallel Mode)\n",
    "\n",
    "What is multi-source? In the examples above, interventions are at most across two examples. We support interventions across many examples. You can sample representations from two inputs, and plut them into a single **Base**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "847410a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "_the                 0.07233363389968872\n",
      "_a                   0.05731499195098877\n",
      "_not                 0.04443885385990143\n",
      "_Italian             0.033642884343862534\n",
      "_often               0.024385808035731316\n",
      "_called              0.022171705961227417\n",
      "_known               0.017808808013796806\n",
      "_that                0.016059240326285362\n",
      "_\"                   0.012973357923328876\n",
      "_an                  0.012878881767392159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "parallel_config = pv.IntervenableConfig([\n",
    "  {\"layer\": 3, \"component\": \"block_output\"},\n",
    "  {\"layer\": 3, \"component\": \"block_output\"}],\n",
    "  # intervene on base at the same time\n",
    "  mode=\"parallel\")\n",
    "parallel_gpt2 = pv.IntervenableModel(\n",
    "  parallel_config, model=gpt2)\n",
    "base = tokenizer(\n",
    "  \"The capital of Spain is\", \n",
    "  return_tensors=\"pt\")\n",
    "sources = [\n",
    "  tokenizer(\"The language of Spain is\", \n",
    "    return_tensors=\"pt\"),\n",
    "  tokenizer(\"The capital of Italy is\", \n",
    "    return_tensors=\"pt\")]\n",
    "intervened_outputs = parallel_gpt2(\n",
    "    base, sources,\n",
    "    {\"sources->base\": (\n",
    "    # each list has a dimensionality of\n",
    "    # [num_intervention, batch, num_unit]\n",
    "    [[[1]],[[3]]],  [[[1]],[[3]]])}\n",
    ")\n",
    "\n",
    "distrib = pv.embed_to_distrib(\n",
    "    gpt2, intervened_outputs[1].last_hidden_state, logits=False)\n",
    "pv.top_vals(tokenizer, distrib[0][-1], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93402c",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention (Serial Mode)\n",
    "\n",
    "Or you can do them sequentially, where you intervene among your **Source** examples, and get some intermediate states before merging the activations into the **Base** run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e5752dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_the                 0.06737838685512543\n",
      "_a                   0.059834375977516174\n",
      "_not                 0.04629501700401306\n",
      "_Italian             0.03623826056718826\n",
      "_often               0.021700192242860794\n",
      "_called              0.01840786263346672\n",
      "_that                0.0157712884247303\n",
      "_known               0.014391838572919369\n",
      "_an                  0.013535155914723873\n",
      "_very                0.013022392988204956\n"
     ]
    }
   ],
   "source": [
    "config = pv.IntervenableConfig([\n",
    "  {\"layer\": 3, \"component\": \"block_output\"},\n",
    "  {\"layer\": 10, \"component\": \"block_output\"}],\n",
    "  # intervene on base one after another\n",
    "  mode=\"serial\")\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "  config, model=gpt2)\n",
    "base = tokenizer(\n",
    "  \"The capital of Spain is\", \n",
    "  return_tensors=\"pt\")\n",
    "sources = [\n",
    "  tokenizer(\"The language of Spain is\", \n",
    "    return_tensors=\"pt\"),\n",
    "  tokenizer(\"The capital of Italy is\", \n",
    "    return_tensors=\"pt\")]\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources,\n",
    "    # intervene in serial at two positions\n",
    "    {\"source_0->source_1\": 1, \n",
    "     \"source_1->base\"    : 4})\n",
    "\n",
    "distrib = pv.embed_to_distrib(\n",
    "    gpt2, intervened_outputs[1].last_hidden_state, logits=False)\n",
    "pv.top_vals(tokenizer, distrib[0][-1], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28621880",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention with Subspaces (Parallel Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773aba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": \n",
    "         [[0, 128], [128, 256]]}]*2,\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    "    # act in parallel\n",
    "    mode=\"parallel\"\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"The capital of Italy is\", return_tensors=\"pt\"),\n",
    "          tokenizer(\"The capital of China is\", return_tensors=\"pt\")]\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources,\n",
    "    # on same position\n",
    "    {\"sources->base\": 4},\n",
    "    # on different subspaces\n",
    "    subspaces=[[[0]], [[1]]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223603f",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention with Subspaces (Serial Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305e0607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]]},\n",
    "    {\"layer\": 2, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]]}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    "    # act in parallel\n",
    "    mode=\"serial\"\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"The capital of Italy is\", return_tensors=\"pt\"),\n",
    "          tokenizer(\"The capital of China is\", return_tensors=\"pt\")]\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources,\n",
    "    # serialized intervention\n",
    "    # order is based on sources list\n",
    "    {\"source_0->source_1\": 3, \"source_1->base\": 4},\n",
    "    # on different subspaces\n",
    "    subspaces=[[[0]], [[1]]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5fcb37",
   "metadata": {},
   "source": [
    "### Interchange Intervention Training (IIT)\n",
    "Interchange intervention training (IIT) is a technique of inducing causal structures into neural models. This library naturally supports this. By training IIT, you can simply turn the gradient on for the wrapping model. In this way, your model can be trained with your interventional signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c7dde89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "    \"layer\": 8}, \n",
    "    model=gpt2\n",
    ")\n",
    "\n",
    "pv_gpt2.enable_model_gradients()\n",
    "# run counterfactual forward as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7ccad",
   "metadata": {},
   "source": [
    "## pyvene 102\n",
    "Now, you are pretty familiar with pyvene basic APIs. There are more to come. We support all sorts of weird interventions, and we encapsulate them as objects so that, even they are super weird (e.g., nested, multiple locations, different types), you can share them easily with others. BTW, if the intervention is trainable, the artifacts will be saved and shared as well.\n",
    "\n",
    "With that, here are a couple of additional APIs.\n",
    "\n",
    "### Grouping\n",
    "\n",
    "You can group interventions together so that they always receive the same input when you want to use them to get activations at different places. Here is an example, where you are taking in the same **Source** example, you fetch activations twice: once in position 3 and layer 0, once in position 4 and layer 2. You don't have to pass in another dummy **Source**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84afd62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    {\"layer\": 0, \"component\": \"block_output\", \"group_key\": 0},\n",
    "    {\"layer\": 2, \"component\": \"block_output\", \"group_key\": 0}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")]\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources, \n",
    "    {\"sources->base\": ([\n",
    "        [[3]], [[4]] # these two are for two interventions\n",
    "    ], [             # source position 3 into base position 4\n",
    "        [[3]], [[4]] \n",
    "    ])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aeb892",
   "metadata": {},
   "source": [
    "### Intervention Skipping in Runtime\n",
    "You may configure a lot of interventions, but during training, not every example will have to use all of them. So, you can skip interventions for different examples differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cd8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    # these are equivalent interventions\n",
    "    # we create them on purpose\n",
    "    {\"layer\": 0, \"component\": \"block_output\"},\n",
    "    {\"layer\": 0, \"component\": \"block_output\"},\n",
    "    {\"layer\": 0, \"component\": \"block_output\"}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")\n",
    "# skipping 1, 2 and 3\n",
    "_, pv_out1 = pv_gpt2(base, [None, None, source],\n",
    "    {\"sources->base\": ([None, None, [[4]]], [None, None, [[4]]])})\n",
    "_, pv_out2 = pv_gpt2(base, [None, source, None],\n",
    "    {\"sources->base\": ([None, [[4]], None], [None, [[4]], None])})\n",
    "_, pv_out3 = pv_gpt2(base, [source, None, None],\n",
    "    {\"sources->base\": ([[[4]], None, None], [[[4]], None, None])})\n",
    "# should have the same results\n",
    "print(\n",
    "    torch.equal(pv_out1.last_hidden_state, pv_out2.last_hidden_state),\n",
    "    torch.equal(pv_out2.last_hidden_state, pv_out3.last_hidden_state)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df6acd",
   "metadata": {},
   "source": [
    "### Subspace Partition\n",
    "You can partition your subspace before hand. If you don't, the library assumes you each neuron is in its own subspace. In this example, you partition your subspace into two continous chunk, `[0, 128), [128,256)`, which means all the neurons from index 0 upto 127 are along to partition 1. During runtime, you can intervene on all the neurons in the same parition together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a66bbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    # they are linked to manipulate the same representation\n",
    "    # but in different subspaces\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     # subspaces can be partitioned into continuous chunks\n",
    "     # [i, j] are the boundary indices\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]]}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")\n",
    "\n",
    "# using intervention skipping for subspace\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, [source],\n",
    "    {\"sources->base\": 4},\n",
    "    # intervene only only dimensions from 128 to 256\n",
    "    subspaces=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdde257",
   "metadata": {},
   "source": [
    "### Intervention Linking\n",
    "Interventions can be linked to share weights and share subspaces. Here is an example of how to link interventions together. If interventions are trainable, then their weights are tied as well.\n",
    "\n",
    "Why this is useful? it is because sometimes, you may want to intervene on different subspaces differently. Say you have a representation in a size of 512, and you hypothesize the first half represents A, and the second half represents B, you can then use the subspace intervention to test it out. With trainable interventions, you can also optimize your interventions on the same representation yet with different subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec19da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    # they are linked to manipulate the same representation\n",
    "    # but in different subspaces\n",
    "    {\"layer\": 0, \"component\": \"block_output\", \n",
    "     \"subspace_partition\": [[0, 128], [128, 256]], \"intervention_link_key\": 0},\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]], \"intervention_link_key\": 0}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")\n",
    "\n",
    "# using intervention skipping for subspace\n",
    "_, pv_out1 = pv_gpt2(\n",
    "    base, [None, source],\n",
    "    # 4 means token position 4\n",
    "    {\"sources->base\": ([None, [[4]]], [None, [[4]]])},\n",
    "    # 1 means the second partition in the config\n",
    "    subspaces=[None, [[1]]],\n",
    ")\n",
    "_, pv_out2 = pv_gpt2(\n",
    "    base,\n",
    "    [source, None],\n",
    "    {\"sources->base\": ([[[4]], None], [[[4]], None])},\n",
    "    subspaces=[[[1]], None],\n",
    ")\n",
    "print(torch.equal(pv_out1.last_hidden_state, pv_out2.last_hidden_state))\n",
    "\n",
    "# subspaces provide a list of index and they can be in any order\n",
    "_, pv_out3 = pv_gpt2(\n",
    "    base,\n",
    "    [source, source],\n",
    "    {\"sources->base\": ([[[4]], [[4]]], [[[4]], [[4]]])},\n",
    "    subspaces=[[[0]], [[1]]],\n",
    ")\n",
    "_, pv_out4 = pv_gpt2(\n",
    "    base,\n",
    "    [source, source],\n",
    "    {\"sources->base\": ([[[4]], [[4]]], [[[4]], [[4]]])},\n",
    "    subspaces=[[[1]], [[0]]],\n",
    ")\n",
    "print(torch.equal(pv_out3.last_hidden_state, pv_out4.last_hidden_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b7a3e",
   "metadata": {},
   "source": [
    "### Add New Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acce6e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "# get a flan-t5 from HuggingFace\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "config = T5Config.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-small\", config=config\n",
    ")\n",
    "\n",
    "# config the intervention mapping with pv global vars\n",
    "\"\"\"Only define for the block output here for simplicity\"\"\"\n",
    "pv.type_to_module_mapping[type(t5)] = {\n",
    "    \"mlp_output\": (\"encoder.block[%s].layer[1]\", \n",
    "                   pv.models.constants.CONST_OUTPUT_HOOK),\n",
    "    \"attention_input\": (\"encoder.block[%s].layer[0]\", \n",
    "                        pv.models.constants.CONST_OUTPUT_HOOK),\n",
    "}\n",
    "pv.type_to_dimension_mapping[type(t5)] = {\n",
    "    \"mlp_output\": (\"d_model\",),\n",
    "    \"attention_input\": (\"d_model\",),\n",
    "    \"block_output\": (\"d_model\",),\n",
    "    \"head_attention_value_output\": (\"d_model/num_heads\",),\n",
    "}\n",
    "\n",
    "# wrap as gpt2\n",
    "pv_t5 = pv.IntervenableModel({\n",
    "    \"layer\": 0,\n",
    "    \"component\": \"mlp_output\",\n",
    "    \"source_representation\": torch.zeros(\n",
    "        t5.config.d_model)\n",
    "}, model=t5)\n",
    "\n",
    "# then intervene!\n",
    "base = tokenizer(\"The capital of Spain is\", \n",
    "                 return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenizer(\n",
    "    \"\", return_tensors=\"pt\").input_ids\n",
    "base[\"decoder_input_ids\"] = decoder_input_ids\n",
    "intervened_outputs = pv_t5(\n",
    "    base, \n",
    "    unit_locations={\"base\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba158a92",
   "metadata": {},
   "source": [
    "### Composing Complex Intervention Schema: Path Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51cadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "Directory './tmp/' already exists.\n"
     ]
    }
   ],
   "source": [
    "import pyvene as pv\n",
    "\n",
    "def path_patching_config(\n",
    "    layer, last_layer, \n",
    "    component=\"head_attention_value_output\", unit=\"h.pos\"\n",
    "):\n",
    "    intervening_component = [\n",
    "        {\"layer\": layer, \"component\": component, \"unit\": unit, \"group_key\": 0}]\n",
    "    restoring_components = []\n",
    "    if not component.startswith(\"mlp_\"):\n",
    "        restoring_components += [\n",
    "            {\"layer\": layer, \"component\": \"mlp_output\", \"group_key\": 1}]\n",
    "    for i in range(layer+1, last_layer):\n",
    "        restoring_components += [\n",
    "            {\"layer\": i, \"component\": \"attention_output\", \"group_key\": 1},\n",
    "            {\"layer\": i, \"component\": \"mlp_output\", \"group_key\": 1}\n",
    "        ]\n",
    "    intervenable_config = pv.IntervenableConfig(\n",
    "        intervening_component + restoring_components)\n",
    "    return intervenable_config\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    path_patching_config(4, gpt2.config.n_layer), \n",
    "    model=gpt2\n",
    ")\n",
    "\n",
    "pv_gpt2.save(\n",
    "    save_directory=\"./tmp/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9074f716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n"
     ]
    }
   ],
   "source": [
    "pv_gpt2 = pv.IntervenableModel.load(\n",
    "    \"./tmp/\",\n",
    "    model=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546e858",
   "metadata": {},
   "source": [
    "### Composing Complex Intervention Schema: Causal Tracing in 15 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b6a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import pyvene as pv\n",
    "\n",
    "def causal_tracing_config(\n",
    "  l, c=\"mlp_activation\", w=10, tl=48):\n",
    "  s = max(0, l - w // 2)\n",
    "  e = min(tl, l - (-w // 2))\n",
    "  config = pv.IntervenableConfig(\n",
    "    [{\"component\": \"block_input\"}] + \n",
    "    [{\"layer\": l, \"component\": c} \n",
    "      for l in range(s, e)],\n",
    "    [pv.NoiseIntervention] +\n",
    "    [pv.VanillaIntervention]*(e-s))\n",
    "  return config\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    causal_tracing_config(4), \n",
    "    model=gpt2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6eb49d",
   "metadata": {},
   "source": [
    "### The End\n",
    "Now you are graduating from pyvene 101! Feel free to take a look at our tutorials for more challenging interventions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
