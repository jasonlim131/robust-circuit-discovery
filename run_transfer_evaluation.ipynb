{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import *\n",
    "from logic_data.utils import *\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    is_notebook = False\n",
    "    try:\n",
    "        cmd = argparse.ArgumentParser('The testing components of')\n",
    "        cmd.add_argument('--gpu', default=-1, type=int, help='use id of gpu, -1 if cpu.')\n",
    "        cmd.add_argument('--train_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--eval_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--lr', default=0.01, type=float, help='learning rate')\n",
    "        cmd.add_argument('--data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument('--train_data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument('--test_data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument(\n",
    "            '--encoder_config_path', \n",
    "            type=str, help='path to the encoder config'\n",
    "        )\n",
    "        cmd.add_argument(\n",
    "            '--decoder_config_path', \n",
    "            type=str, help='path to the decoder config'\n",
    "        )\n",
    "        cmd.add_argument('--max_seq_len', default=512, type=int)\n",
    "        cmd.add_argument('--seed', default=42, type=int)\n",
    "        cmd.add_argument('--gradient_accumulation_steps', default=1, type=int)\n",
    "        cmd.add_argument('--output_dir', required=True, type=str, help='save dir')\n",
    "        cmd.add_argument('--local_rank', default=-1, type=int, help='multi gpu training')\n",
    "        cmd.add_argument('--epochs', default=10, type=int, help='training epochs')\n",
    "        cmd.add_argument('--model_path', type=str, required=False, default=None)\n",
    "        cmd.add_argument('--warm_up', type=float, default=0.1)\n",
    "        cmd.add_argument('--is_wandb', default=False, action='store_true')\n",
    "        cmd.add_argument('--log_step', default=10, type=int)\n",
    "        cmd.add_argument('--valid_steps', default=500, type=int)\n",
    "        cmd.add_argument('--early_stopping', default=5, type=int)\n",
    "        cmd.add_argument('--device', default=\"cuda\", type=str, help='')\n",
    "        cmd.add_argument('--do_prealign_eval', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_align', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_eval', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_test', default=False, action='store_true')\n",
    "        \n",
    "        cmd.add_argument('--n_training_program', default=5, type=int)\n",
    "        cmd.add_argument('--n_fewshot', default=6, type=int)\n",
    "        cmd.add_argument('--aligning_layer_n', default=0, type=int)\n",
    "        \n",
    "        args = cmd.parse_args(sys.argv[1:])\n",
    "    except:\n",
    "        is_notebook = True\n",
    "        parser = argparse.ArgumentParser()\n",
    "        args = parser.parse_args([])\n",
    "        args.gpu = 1\n",
    "        args.eval_batch_size = 64\n",
    "        args.gradient_accumulation_steps = 2\n",
    "        args.data_path = \"./logic_data\"\n",
    "        args.encoder_config_path = None\n",
    "        args.decoder_config_path = None\n",
    "        args.max_seq_len = 512\n",
    "        args.output_dir = \"./results_notebook/\"\n",
    "        args.epochs = 10\n",
    "        args.warm_up = 0.1\n",
    "        args.is_wandb = False\n",
    "        args.log_step = 10\n",
    "        args.valid_steps = 100 # -1 not do training eval!\n",
    "        args.early_stopping = 999 # large == never early stop!\n",
    "        args.device = \"cuda:0\"\n",
    "        args.do_prealign_eval = True # do it once at least!\n",
    "        args.do_align = True\n",
    "        args.do_eval = True\n",
    "        args.do_test = True\n",
    "        # args.model_path = None\n",
    "        \n",
    "        # alignment search setting\n",
    "        args.aligning_layer_n = 0\n",
    "        args.aligning_basis_n = 600\n",
    "        args.aligning_var_n = 1\n",
    "        \n",
    "        print(\"Using in a notebook env.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8de782",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e32900",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_training_clauses = [\n",
    "    '( c != a ) or ( a == b )',\n",
    "    '( c != a ) and ( a == b )',\n",
    "    '( c != a ) or ( a != b )',\n",
    "    '( c != a ) and ( b != c )',\n",
    "    '( c != a ) and ( a != b )',\n",
    "    '( c == a ) or ( a != b )',\n",
    "    '( c == a ) or ( b != c )',\n",
    "    '( c == a ) or ( b == c )',\n",
    "    '( b != a ) and ( b != c )',\n",
    "    '( c == a ) and ( b != c )',\n",
    "    '( c == a ) or ( a == b )',\n",
    "]\n",
    "\n",
    "target_training_clauses = [\n",
    "    '( c != a ) or ( a == b )',\n",
    "    '( c != a ) and ( a == b )',\n",
    "    '( c != a ) or ( a != b )',\n",
    "    '( c != a ) and ( b != c )',\n",
    "    '( c != a ) and ( a != b )',\n",
    "    '( c == a ) or ( a != b )',\n",
    "    '( c == a ) or ( b != c )',\n",
    "    '( c == a ) or ( b == c )',\n",
    "    '( b != a ) and ( b != c )',\n",
    "    '( c == a ) and ( b != c )',\n",
    "    '( c == a ) or ( a == b )',\n",
    "]\n",
    "align_type = \"left_aligment\"\n",
    "level = \"l2\"\n",
    "shared_train = True if level == \"l1\" else False\n",
    "n_fewshot = 10\n",
    "n_examples = n_fewshot + 1\n",
    "n_testing_examples = 1000\n",
    "control = True\n",
    "\n",
    "for seed in [66,]: # [42, 66, 77, 88, 99]\n",
    "    for source_clauses in source_training_clauses:\n",
    "        for clauses in target_training_clauses:\n",
    "            args.seed = seed\n",
    "            set_seed(args.seed)\n",
    "            source_clauses_no_space = source_clauses.replace(\" \", \"\")\n",
    "            clauses_no_space = clauses.replace(\" \", \"\")\n",
    "            print(f\"source clauses: {source_clauses}; transferring clauses: {clauses}\")\n",
    "            print(\"seed: \", seed)\n",
    "            # finding model files, it needs to exist on the disk.\n",
    "            if control:\n",
    "                model_name = f\"logic_pipeline.model.gpt2.n_rule.11.n_shot.{n_fewshot}.seed.{seed}/model-last\"\n",
    "            else:\n",
    "                model_name = f\"logic_pipeline.model.gpt2.n_rule.11.n_shot.{n_fewshot}.seed.{seed}.clauses.{source_clauses_no_space}.align.{align_type}.seed.{seed}/model-last\"\n",
    "            args.model_path = os.path.join(args.output_dir, model_name)\n",
    "            logger = logging.getLogger()\n",
    "            aligment_sampled_data = left_aligment_sampler(\n",
    "                clauses, n_testing_examples, n_examples = n_examples, shared_train=shared_train\n",
    "            )\n",
    "            test_data = {\n",
    "                \"base_input_ids\" : aligment_sampled_data[0],\n",
    "                \"base_output_ids\" : aligment_sampled_data[1],\n",
    "                \"source_input_ids\" : aligment_sampled_data[3],\n",
    "                \"source_output_ids\" : aligment_sampled_data[4],\n",
    "                \"counterfacut_output_ids\": aligment_sampled_data[6],\n",
    "                \"clauses\" : aligment_sampled_data[2],\n",
    "                \"intervention_ids\": [0 for i in range(len(aligment_sampled_data[0]))]\n",
    "            }\n",
    "            test_dataset = Dataset.from_dict(\n",
    "                {\n",
    "                    \"input_ids\": test_data[\"base_input_ids\"], \n",
    "                    \"labels\": test_data[\"base_output_ids\"],\n",
    "                    \"source_input_ids\": test_data[\"source_input_ids\"], \n",
    "                    \"counterfactual_labels\": test_data[\"counterfacut_output_ids\"],\n",
    "                    \"intervention_ids\": test_data[\"intervention_ids\"],\n",
    "                }\n",
    "            ).with_format(\"torch\")\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=args.eval_batch_size)\n",
    "\n",
    "            # Model\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            configuration = GPT2Config.from_pretrained(os.path.join(args.data_path, \"decoder_config.json\"))\n",
    "\n",
    "            if \"logic\" in model_name:\n",
    "                arity = 3\n",
    "            start_idx = 1 + (arity + 4) * int(n_fewshot)\n",
    "            end_idx = start_idx + (arity+1)\n",
    "            alignment_config = {\n",
    "                \"layer\" : args.aligning_layer_n,\n",
    "                \"token_range\" : [start_idx, end_idx] # this is kind of fixed?\n",
    "            }\n",
    "            if args.aligning_var_n == 1:\n",
    "                intervention_config = {\n",
    "                    0: [[0, args.aligning_basis_n]]\n",
    "                }\n",
    "            elif args.aligning_var_n == 2:\n",
    "                pass\n",
    "            logging.info(f\"intervention_config = {intervention_config}\")\n",
    "            logging.info(f\"alignment_config = {alignment_config}\")\n",
    "\n",
    "            model = AlignableGPT2LMHeadModel(configuration, alignment_config=alignment_config)\n",
    "            if args.model_path is not None:\n",
    "                logging.info(\"Loading pretrained model.\")\n",
    "                raw_weights = torch.load(os.path.join(args.model_path, 'pytorch_model.bin'))\n",
    "                model.load_state_dict(raw_weights, strict=False)\n",
    "\n",
    "            # we need to set off gradients!\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            device = torch.device(args.device)\n",
    "            if \"cuda:\" not in args.device:\n",
    "                n_gpu = torch.cuda.device_count()\n",
    "                logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "            else:\n",
    "                n_gpu = 1\n",
    "                logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "\n",
    "            if n_gpu > 1:\n",
    "                model = torch.nn.DataParallel(model)\n",
    "            _ = model.to(device)\n",
    "\n",
    "            aligner = LogicSolverAligner(\n",
    "                model, device=device, \n",
    "                logger=logger,\n",
    "                is_master=True, \n",
    "                n_gpu=n_gpu,\n",
    "                is_wandb=args.is_wandb, \n",
    "                model_name=model_name,\n",
    "                intervention_config=intervention_config\n",
    "            )\n",
    "            num_params = count_parameters(model)\n",
    "            logging.info(f'Number of {model_name} model params: {num_params}')\n",
    "\n",
    "            total_count = 0\n",
    "            correct_count = 0\n",
    "            _ = model.eval()\n",
    "            epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                input_ids = inputs['input_ids'].to(device)\n",
    "                labels = inputs['labels'].to(device)\n",
    "                outputs = model(input_ids=input_ids)\n",
    "                actual_test_labels = labels[:, -3]\n",
    "                pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "                correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "                total_count += len(correct_labels)\n",
    "                correct_count += correct_labels.sum().tolist()\n",
    "\n",
    "                current_acc = round(correct_count/total_count, 2)\n",
    "                epoch_iterator.set_postfix({'acc': current_acc})\n",
    "            task_acc = round(correct_count/total_count, 2)\n",
    "\n",
    "            total_count = 0\n",
    "            correct_count = 0\n",
    "            aligner.model.eval()\n",
    "            epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                for k, v in inputs.items():\n",
    "                    if v is not None and isinstance(v, torch.Tensor):\n",
    "                        inputs[k] = v.to(device)\n",
    "                if aligner.preload_intervention_corr is not None:\n",
    "                    intervention_corr = aligner.preload_intervention_corr.expand(\n",
    "                        inputs['input_ids'].shape[0],-1\n",
    "                    ).to(device)\n",
    "                else:\n",
    "                    assert False # not implemented\n",
    "\n",
    "                # aligning forward!\n",
    "                source_hidden_states = aligner.model(\n",
    "                   input_ids=inputs['source_input_ids']\n",
    "                ).rotated_hidden_states\n",
    "                outputs = aligner.model(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    source_hidden_states=source_hidden_states,\n",
    "                    intervention_corr=intervention_corr,\n",
    "                    labels=inputs['counterfactual_labels']\n",
    "                )\n",
    "\n",
    "                actual_test_labels = inputs['counterfactual_labels'][:, -3]\n",
    "                pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "                correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "                total_count += len(correct_labels)\n",
    "                correct_count += correct_labels.sum().tolist()\n",
    "\n",
    "                current_acc = round(correct_count/total_count, 2)\n",
    "                epoch_iterator.set_postfix({'acc': current_acc})\n",
    "            iia_acc = round(correct_count/total_count, 2)\n",
    "\n",
    "            results += [[source_clauses, clauses, seed, task_acc, iia_acc]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22885c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results, columns = [\n",
    "    'src_clauses', 'tgt_clauses', 'seed', 'task_acc', 'iit_acc'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_df = result_df.pivot(index='src_clauses', columns='tgt_clauses', values='iit_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap_df, annot=True, fmt=\"g\", cmap='viridis', vmax=1.0, vmin=0.0)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55888376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
