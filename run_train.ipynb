{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3036982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, argparse, sys, pickle, time\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from models.modelings_gpt2 import *\n",
    "from logic_data.constants import *\n",
    "from datasets import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439df962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicSolverTrainer(object):\n",
    "    def __init__(\n",
    "        self, model,\n",
    "        is_master,\n",
    "        device,\n",
    "        logger,\n",
    "        lr=5e-5,\n",
    "        apex_enable=False,\n",
    "        n_gpu=1,\n",
    "        early_stopping=5,\n",
    "        do_statistic=False,\n",
    "        is_wandb=False,\n",
    "        model_name=\"\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.is_master = is_master\n",
    "        self.logger = logger\n",
    "        self.is_wandb = is_wandb\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.n_gpu = n_gpu\n",
    "    \n",
    "        self.early_stopping = early_stopping\n",
    "    \n",
    "    def train(\n",
    "        self, train_dataloader, dev_dataloader,\n",
    "        optimizer, scheduler, output_dir,\n",
    "        log_step, valid_steps, epochs, \n",
    "        gradient_accumulation_steps,\n",
    "    ):\n",
    "        self.model.train()\n",
    "        train_iterator = trange(\n",
    "            0, int(epochs), desc=\"Epoch\"\n",
    "        )\n",
    "        total_step = 0\n",
    "        total_log_step = 0\n",
    "        best_eval_acc = -1\n",
    "        for epoch in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True)\n",
    "            for step, inputs in enumerate(epoch_iterator):\n",
    "                for k, v in inputs.items():\n",
    "                    if v is not None and isinstance(v, torch.Tensor):\n",
    "                        inputs[k] = v.to(self.device)\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs.loss.mean() if self.n_gpu > 1 else outputs.loss\n",
    "                \n",
    "                actual_test_labels = inputs['labels'][:, -3]\n",
    "                pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "                correct_labels = (actual_test_labels==pred_test_labels)\n",
    "                \n",
    "                step_accuracy = correct_labels.sum() / correct_labels.shape[0]\n",
    "                step_accuracy = step_accuracy.tolist()\n",
    "\n",
    "                if total_step % log_step == 0 and self.is_wandb:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"train/loss\": loss.item(),\n",
    "                            \"train/step_accuracy\": step_accuracy\n",
    "                        },\n",
    "                        step=total_log_step\n",
    "                    )\n",
    "                    \n",
    "                    if total_step % valid_steps == 0:\n",
    "                        total_count = 0\n",
    "                        correct_count = 0\n",
    "                        self.model.eval()\n",
    "                        for step, inputs in enumerate(dev_dataloader):\n",
    "                            for k, v in inputs.items():\n",
    "                                if v is not None and isinstance(v, torch.Tensor):\n",
    "                                    inputs[k] = v.to(self.device)\n",
    "                            outputs = model(**inputs)\n",
    "\n",
    "                            actual_test_labels = inputs['labels'][:, -3]\n",
    "                            pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "                            correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "                            total_count += len(correct_labels)\n",
    "                            correct_count += correct_labels.sum().tolist()\n",
    "\n",
    "                        current_acc = round(correct_count/total_count, 2)\n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                \"eval/accuracy\": current_acc\n",
    "                            },\n",
    "                            step=total_log_step\n",
    "                        )\n",
    "                        if current_acc > best_eval_acc:\n",
    "                            best_eval_acc = current_acc\n",
    "                            if self.is_master:\n",
    "                                if self.n_gpu > 1:\n",
    "                                    self.model.module.save_pretrained(os.path.join(output_dir, 'model-best'))\n",
    "                                else:\n",
    "                                    self.model.save_pretrained(os.path.join(output_dir, 'model-best'))\n",
    "                        self.model.train()\n",
    "                        \n",
    "                    \n",
    "                    total_log_step += 1\n",
    "                loss_str = round(loss.item(), 2)\n",
    "                epoch_iterator.set_postfix({'loss': loss_str})\n",
    "                \n",
    "                if gradient_accumulation_steps > 1:\n",
    "                    loss = loss / gradient_accumulation_steps\n",
    "                \n",
    "                if total_step % gradient_accumulation_steps == 0:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    self.model.zero_grad()\n",
    "                    \n",
    "                total_step += 1\n",
    "                \n",
    "        logging.info(\"Training is finished ...\") \n",
    "        if self.is_master:\n",
    "            if self.n_gpu > 1:\n",
    "                self.model.module.save_pretrained(os.path.join(output_dir, 'model-last'))\n",
    "            else:\n",
    "                self.model.save_pretrained(os.path.join(output_dir, 'model-last'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7c3b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in a notebook env.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: The testing components of [-h] [--gpu GPU]\n",
      "                                 [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                                 [--eval_batch_size EVAL_BATCH_SIZE] [--lr LR]\n",
      "                                 --data_path DATA_PATH\n",
      "                                 [--encoder_config_path ENCODER_CONFIG_PATH]\n",
      "                                 [--decoder_config_path DECODER_CONFIG_PATH]\n",
      "                                 [--max_seq_len MAX_SEQ_LEN] [--seed SEED]\n",
      "                                 [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                                 --output_dir OUTPUT_DIR\n",
      "                                 [--local_rank LOCAL_RANK] [--epochs EPOCHS]\n",
      "                                 [--model_path MODEL_PATH] [--warm_up WARM_UP]\n",
      "                                 [--is_wandb] [--log_step LOG_STEP]\n",
      "                                 [--valid_steps VALID_STEPS]\n",
      "                                 [--early_stopping EARLY_STOPPING]\n",
      "                                 [--device DEVICE] [--do_train] [--do_eval]\n",
      "                                 [--do_test]\n",
      "                                 [--n_training_program N_TRAINING_PROGRAM]\n",
      "                                 [--n_fewshot N_FEWSHOT]\n",
      "The testing components of: error: the following arguments are required: --data_path, --output_dir\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    is_notebook = False\n",
    "    try:\n",
    "        cmd = argparse.ArgumentParser('The testing components of')\n",
    "        cmd.add_argument('--gpu', default=-1, type=int, help='use id of gpu, -1 if cpu.')\n",
    "        cmd.add_argument('--train_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--eval_batch_size', default=128, type=int, help='training batch size')\n",
    "        cmd.add_argument('--lr', default=0.01, type=float, help='learning rate')\n",
    "        cmd.add_argument('--data_path', required=True, type=str, help='path to the training corpus')\n",
    "        cmd.add_argument(\n",
    "            '--encoder_config_path', \n",
    "            type=str, help='path to the encoder config'\n",
    "        )\n",
    "        cmd.add_argument(\n",
    "            '--decoder_config_path', \n",
    "            type=str, help='path to the decoder config'\n",
    "        )\n",
    "        cmd.add_argument('--max_seq_len', default=512, type=int)\n",
    "        cmd.add_argument('--seed', default=42, type=int)\n",
    "        cmd.add_argument('--gradient_accumulation_steps', default=1, type=int)\n",
    "        cmd.add_argument('--output_dir', required=True, type=str, help='save dir')\n",
    "        cmd.add_argument('--local_rank', default=-1, type=int, help='multi gpu training')\n",
    "        cmd.add_argument('--epochs', default=10, type=int, help='training epochs')\n",
    "        cmd.add_argument('--model_path', type=str, required=False, default=None)\n",
    "        cmd.add_argument('--warm_up', type=float, default=0.1)\n",
    "        cmd.add_argument('--is_wandb', default=False, action='store_true')\n",
    "        cmd.add_argument('--log_step', default=10, type=int)\n",
    "        cmd.add_argument('--valid_steps', default=500, type=int)\n",
    "        cmd.add_argument('--early_stopping', default=5, type=int)\n",
    "        cmd.add_argument('--device', default=\"cuda\", type=str, help='')\n",
    "        cmd.add_argument('--do_train', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_eval', default=False, action='store_true')\n",
    "        cmd.add_argument('--do_test', default=False, action='store_true')\n",
    "        \n",
    "        cmd.add_argument('--n_training_program', default=5, type=int)\n",
    "        cmd.add_argument('--n_fewshot', default=6, type=int)\n",
    "        \n",
    "        args = cmd.parse_args(sys.argv[1:])\n",
    "    except:\n",
    "        is_notebook = True\n",
    "        parser = argparse.ArgumentParser()\n",
    "        args = parser.parse_args([])\n",
    "        args.gpu = 1\n",
    "        args.train_batch_size = 64\n",
    "        args.eval_batch_size = 64\n",
    "        args.gradient_accumulation_steps = 1\n",
    "        args.lr = 1e-4\n",
    "        args.data_path = \"./logic_data/\"\n",
    "        args.encoder_config_path = None\n",
    "        args.decoder_config_path = None\n",
    "        args.max_seq_len = 512\n",
    "        args.seed = 42\n",
    "        args.output_dir = \"./results_notebook/\"\n",
    "        args.epochs = 200\n",
    "        args.warm_up = 0.1\n",
    "        args.is_wandb = True\n",
    "        args.log_step = 10\n",
    "        args.valid_steps = 100 # -1 not do training eval!\n",
    "        args.early_stopping = 999 # large == never early stop!\n",
    "        args.device = \"cuda:0\"\n",
    "        args.do_train = True\n",
    "        args.do_eval = True\n",
    "        args.do_test = True\n",
    "        args.model_path = None\n",
    "        \n",
    "        args.n_training_program = 7\n",
    "        args.n_fewshot = 6\n",
    "        \n",
    "        # args.model_path = \"./results_notebook/logic_pipeline.model.gpt2.n_rule.11.n_shot.6.seed.42/model-last/\"\n",
    "        print(\"Using in a notebook env.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:__Number CUDA Devices: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwuzhengx\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/workspace/ToM-Alignment/wandb/run-20230307_045701-h1x1t5j9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wuzhengx/ToM-DAS-GPT2/runs/h1x1t5j9' target=\"_blank\">logic_pipeline.model.gpt2.n_rule.7.n_shot.6.seed.42</a></strong> to <a href='https://wandb.ai/wuzhengx/ToM-DAS-GPT2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wuzhengx/ToM-DAS-GPT2' target=\"_blank\">https://wandb.ai/wuzhengx/ToM-DAS-GPT2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wuzhengx/ToM-DAS-GPT2/runs/h1x1t5j9' target=\"_blank\">https://wandb.ai/wuzhengx/ToM-DAS-GPT2/runs/h1x1t5j9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of gpt2 model params: 18367800\n",
      "INFO:root:OUTPUT DIR: ./results_notebook/logic_pipeline.model.gpt2.n_rule.7.n_shot.6.seed.42\n",
      "Epoch: 0: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:34<00:00, 16.46it/s, loss=4.29]\n",
      "Epoch: 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.03it/s, loss=0.75]\n",
      "Epoch: 2: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.11it/s, loss=0.54]\n",
      "Epoch: 3: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.21it/s, loss=0.52]\n",
      "Epoch: 4: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.51]\n",
      "Epoch: 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.24it/s, loss=0.5]\n",
      "Epoch: 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.20it/s, loss=0.5]\n",
      "Epoch: 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.01it/s, loss=0.5]\n",
      "Epoch: 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.28it/s, loss=0.5]\n",
      "Epoch: 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.19it/s, loss=0.5]\n",
      "Epoch: 10: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.20it/s, loss=0.51]\n",
      "Epoch: 11: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.51]\n",
      "Epoch: 12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.18it/s, loss=0.5]\n",
      "Epoch: 13: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.24it/s, loss=0.5]\n",
      "Epoch: 14: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.14it/s, loss=0.5]\n",
      "Epoch: 15: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.5]\n",
      "Epoch: 16: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.30it/s, loss=0.5]\n",
      "Epoch: 17: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.5]\n",
      "Epoch: 18: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.29it/s, loss=0.5]\n",
      "Epoch: 19: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.5]\n",
      "Epoch: 20: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.49]\n",
      "Epoch: 21: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.28it/s, loss=0.5]\n",
      "Epoch: 22: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.49]\n",
      "Epoch: 23: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.49]\n",
      "Epoch: 24: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.29it/s, loss=0.49]\n",
      "Epoch: 25: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.18it/s, loss=0.49]\n",
      "Epoch: 26: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.20it/s, loss=0.49]\n",
      "Epoch: 27: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.25it/s, loss=0.49]\n",
      "Epoch: 28: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.11it/s, loss=0.49]\n",
      "Epoch: 29: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.27it/s, loss=0.5]\n",
      "Epoch: 30: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.49]\n",
      "Epoch: 31: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.49]\n",
      "Epoch: 32: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.28it/s, loss=0.49]\n",
      "Epoch: 33: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.49]\n",
      "Epoch: 34: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.49]\n",
      "Epoch: 35: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.25it/s, loss=0.49]\n",
      "Epoch: 36: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.16it/s, loss=0.48]\n",
      "Epoch: 37: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.23it/s, loss=0.5]\n",
      "Epoch: 38: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.49]\n",
      "Epoch: 39: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.23it/s, loss=0.48]\n",
      "Epoch: 41: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.48]\n",
      "Epoch: 42: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.48]\n",
      "Epoch: 43: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.18it/s, loss=0.48]\n",
      "Epoch: 44: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.11it/s, loss=0.48]\n",
      "Epoch: 45: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.24it/s, loss=0.49]\n",
      "Epoch: 46: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.10it/s, loss=0.48]\n",
      "Epoch: 47: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.13it/s, loss=0.47]\n",
      "Epoch: 48: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.30it/s, loss=0.47]\n",
      "Epoch: 49: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.46]\n",
      "Epoch: 50: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.48]\n",
      "Epoch: 51: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.27it/s, loss=0.46]\n",
      "Epoch: 52: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.47]\n",
      "Epoch: 53: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.11it/s, loss=0.46]\n",
      "Epoch: 54: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.26it/s, loss=0.45]\n",
      "Epoch: 55: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.10it/s, loss=0.46]\n",
      "Epoch: 56: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.29it/s, loss=0.46]\n",
      "Epoch: 57: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.46]\n",
      "Epoch: 58: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.45]\n",
      "Epoch: 59: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.29it/s, loss=0.44]\n",
      "Epoch: 60: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.15it/s, loss=0.45]\n",
      "Epoch: 61: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.13it/s, loss=0.43]\n",
      "Epoch: 62: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.25it/s, loss=0.44]\n",
      "Epoch: 63: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.43]\n",
      "Epoch: 64: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.21it/s, loss=0.45]\n",
      "Epoch: 65: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.16it/s, loss=0.44]\n",
      "Epoch: 66: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.43]\n",
      "Epoch: 67: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.30it/s, loss=0.42]\n",
      "Epoch: 68: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.42]\n",
      "Epoch: 69: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.20it/s, loss=0.43]\n",
      "Epoch: 70: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.29it/s, loss=0.41]\n",
      "Epoch: 71: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.16it/s, loss=0.42]\n",
      "Epoch: 72: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.28it/s, loss=0.41]\n",
      "Epoch: 73: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.13it/s, loss=0.42]\n",
      "Epoch: 74: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.08it/s, loss=0.39]\n",
      "Epoch: 75: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.22it/s, loss=0.41]\n",
      "Epoch: 76: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.18it/s, loss=0.4]\n",
      "Epoch: 77: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.17it/s, loss=0.41]\n",
      "Epoch: 78: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.21it/s, loss=0.38]\n",
      "Epoch: 79: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.18it/s, loss=0.41]\n",
      "Epoch: 80: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.12it/s, loss=0.39]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 81: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.24it/s, loss=0.41]\n",
      "Epoch: 82: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:31<00:00, 17.13it/s, loss=0.38]\n",
      "Epoch: 83: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:30<00:00, 17.27it/s, loss=0.38]\n",
      "Epoch: 84:  77%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 1210/1563 [01:11<00:48,  7.30it/s, loss=0.49]"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "run_name = f\"logic_pipeline.model.{model_name}.n_rule.{args.n_training_program}.n_shot.{args.n_fewshot}.seed.{args.seed}\"\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Dataloader\n",
    "train_data = pickle.load(open(os.path.join(args.data_path, f\"train_data.n_rule.{args.n_training_program}.n_shot.{args.n_fewshot}.pkl\"), 'rb'))\n",
    "dev_data = pickle.load(open(os.path.join(args.data_path, f\"dev_data.n_rule.{args.n_training_program}.n_shot.{args.n_fewshot}.pkl\"), 'rb'))\n",
    "test_data = pickle.load(open(os.path.join(args.data_path, f\"test_data.n_rule.{args.n_training_program}.n_shot.{args.n_fewshot}.pkl\"), 'rb'))\n",
    "\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\"input_ids\": train_data[\"input_ids\"], \"labels\": train_data[\"output_ids\"]}\n",
    ").with_format(\"torch\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
    "\n",
    "dev_dataset = Dataset.from_dict(\n",
    "    {\"input_ids\": dev_data[\"input_ids\"], \"labels\": dev_data[\"output_ids\"]}\n",
    ").with_format(\"torch\")\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=args.eval_batch_size)\n",
    "\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\"input_ids\": test_data[\"input_ids\"], \"labels\": test_data[\"output_ids\"]}\n",
    ").with_format(\"torch\")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.eval_batch_size)\n",
    "\n",
    "\n",
    "# Model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "configuration = GPT2Config.from_pretrained(os.path.join(args.data_path, \"decoder_config.json\"))\n",
    "model = CustomizedGPT2LMHeadModel(configuration)\n",
    "\n",
    "if args.model_path is not None:\n",
    "    logging.info(\"Loading pretrained model.\")\n",
    "    raw_weights = torch.load(os.path.join(args.model_path, 'pytorch_model.bin'))\n",
    "    model.load_state_dict(raw_weights)\n",
    "    \n",
    "set_seed(args.seed)\n",
    "device = torch.device(args.device)\n",
    "if \"cuda:\" not in args.device:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "else:\n",
    "    n_gpu = 1\n",
    "    logging.info(f'__Number CUDA Devices: {n_gpu}')\n",
    "\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "_ = model.to(device)\n",
    "\n",
    "t_total = int(len(train_dataloader) * args.epochs)\n",
    "\n",
    "warm_up_steps = args.warm_up * t_total\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=args.lr\n",
    ")\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warm_up_steps,\n",
    "                                            num_training_steps=t_total)\n",
    "is_master = True                                    \n",
    "if not os.path.exists(args.output_dir) and is_master:\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = f\"ToM-DAS\"\n",
    "\n",
    "output_dir = os.path.join(args.output_dir, run_name)\n",
    "if args.do_train and args.is_wandb:\n",
    "    import wandb\n",
    "    run = wandb.init(\n",
    "        project=\"ToM-DAS-GPT2\", \n",
    "        entity=\"wuzhengx\",\n",
    "        name=run_name,\n",
    "    )\n",
    "    wandb.config.update(args)\n",
    "if not os.path.exists(args.output_dir) and is_master:\n",
    "    os.mkdir(args.output_dir)\n",
    "    \n",
    "trainer = LogicSolverTrainer(\n",
    "    model, device=device, \n",
    "    logger=logger,\n",
    "    is_master=is_master, \n",
    "    n_gpu=n_gpu,\n",
    "    is_wandb=args.is_wandb, \n",
    "    model_name=model_name,\n",
    ")\n",
    "num_params = count_parameters(model)\n",
    "logging.info(f'Number of {model_name} model params: {num_params}')\n",
    "\n",
    "# Train\n",
    "if args.do_train:\n",
    "    logging.info(f\"OUTPUT DIR: {output_dir}\")\n",
    "    trainer.train(\n",
    "        train_dataloader, dev_dataloader,\n",
    "        optimizer, scheduler, \n",
    "        log_step=args.log_step, valid_steps=args.valid_steps,\n",
    "        output_dir=output_dir, epochs=args.epochs, \n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.is_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc37b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev\n",
    "if args.do_eval:\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    if args.do_eval:\n",
    "        trainer.model.eval()\n",
    "        epoch_iterator = tqdm(dev_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "        for step, inputs in enumerate(epoch_iterator):\n",
    "            for k, v in inputs.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            actual_test_labels = inputs['labels'][:, -3]\n",
    "            pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "            correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "            total_count += len(correct_labels)\n",
    "            correct_count += correct_labels.sum().tolist()\n",
    "\n",
    "            current_acc = round(correct_count/total_count, 2)\n",
    "            epoch_iterator.set_postfix({'acc': current_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4df396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "if args.do_test:\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    if args.do_test:\n",
    "        trainer.model.eval()\n",
    "        epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "        for step, inputs in enumerate(epoch_iterator):\n",
    "            for k, v in inputs.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(device)\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            actual_test_labels = inputs['labels'][:, -3]\n",
    "            pred_test_labels = torch.argmax(outputs.logits[:, -4], dim=-1)\n",
    "            correct_labels = (actual_test_labels==pred_test_labels)\n",
    "\n",
    "            total_count += len(correct_labels)\n",
    "            correct_count += correct_labels.sum().tolist()\n",
    "\n",
    "            current_acc = round(correct_count/total_count, 2)\n",
    "            epoch_iterator.set_postfix({'acc': current_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62212779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
