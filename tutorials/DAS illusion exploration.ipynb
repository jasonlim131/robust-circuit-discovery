{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0954d10",
   "metadata": {},
   "source": [
    "## Exploring illusion in the wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4e6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Zhengxuan Wu\"\n",
    "__version__ = \"11/28/2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf06e7a",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Recently, [a paper](https://openreview.net/forum?id=Ebt7JgMHv1) claims to find DAS creates \"illusions\" with LLMs. Here, we study the \"illusion\" with a very simple setting, a single weight matrix.\n",
    "\n",
    "We explore what is the \"illusion\" found in the paper, and how we can fix the \"illusion\" post-hoc, and why \"illusion\" is actually less about DAS and more about NN itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb3588",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15d946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd8177",
   "metadata": {},
   "source": [
    "### Simulating \n",
    "- `W_out`: a single MLP_out layer\n",
    "- `das_mlp8`: DAS learned unit vector\n",
    "- `das_mlp8_row` and `das_mlp8_null`: rowspace and nullspace projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9a8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate W_out and das directions\n",
    "das_dimension = 1\n",
    "W_out  = torch.nn.Linear(3072, 768).weight.T\n",
    "das_mlp8 = torch.nn.utils.parametrizations.orthogonal(\n",
    "    torch.nn.Linear(3072, das_dimension)).weight\n",
    "\n",
    "# copy from the illusion code\n",
    "Q, _ = torch.linalg.qr(W_out)\n",
    "das_mlp8_row = das_mlp8 @ Q @ Q.T\n",
    "das_mlp8_null = das_mlp8 - das_mlp8_row\n",
    "das_row_unit = das_mlp8_row / das_mlp8_row.norm()\n",
    "das_null_unit = das_mlp8_null / das_mlp8_null.norm()\n",
    "\n",
    "###\n",
    "# DAS in one line\n",
    "# b: base activations\n",
    "# s: source activations\n",
    "# v: DAS learned directions\n",
    "###\n",
    "do_das = lambda b, s, v: b + ((s @ v.T - b @ v.T) @ v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013f7ba",
   "metadata": {},
   "source": [
    "### Some equations\n",
    "\n",
    "\n",
    "1) We know this\n",
    "\n",
    "  ```\n",
    "  (b + (s @ v_null.T - b @ v_null.T) @ v_null) @ W_out = b @ W_out\n",
    "  ```\n",
    "  \n",
    "  as `ANYTHING @ v_null @ W_out = 0`\n",
    "\n",
    "2) Now, let's decompose `v` as `v = v_null + v_row`,\n",
    "\n",
    "  ```\n",
    "  (b + (s @ (v_null + v_row).T - b @ (v_null + v_row).T) @ (v_null + v_row)) @ W_out\n",
    "  ```\n",
    "  \n",
    "  We multiply in `.T`, rewrite as,\n",
    "  \n",
    "  ```\n",
    "  (b + ((s @ v_null.T - b @ v_null.T) + (s @ v_row.T - b @ v_row.T)) @ (v_null + v_row)) @ W_out\n",
    "  ```\n",
    "  \n",
    "  Rewrite the eqn above as,\n",
    "  \n",
    "  ```\n",
    "  (b + \n",
    "    ((s @ v_null.T - b @ v_null.T)) @ v_null + \n",
    "    ((s @ v_row.T - b @ v_row.T))   @ v_null + \n",
    "    ((s @ v_null.T - b @ v_null.T)) @ v_row + \n",
    "    ((s @ v_row.T - b @ v_row.T))   @ v_row + \n",
    "  ) @ W_out\n",
    "  ```\n",
    "  \n",
    "  We know `ANYTHING @ v_null @ W_out = 0`. Thus, we LHS and RHS as,\n",
    "  \n",
    "  ```\n",
    "  (b + (s @ v.T - b @ v.T) @ v) @ W_out = \n",
    "  (b + \n",
    "    (s @ v_row.T - b @ v_row.T) @ v_row + \n",
    "    (s @ v_null.T - b @ v_null.T) @ v_row + \n",
    "  ) @ W_out\n",
    "  ```\n",
    "  \n",
    "3) The diff between intervening with `v` and `v_row` has to come from `(s @ v_null.T - b @ v_null.T)` being non-zero.\n",
    "\n",
    "\n",
    "**remark:** this is different from just saying removing `v_null` from `v` changes the effect magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1fcc9",
   "metadata": {},
   "source": [
    "### Simulating different intervention results just for the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfdedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two random activations\n",
    "b_act = torch.rand(1, 3072)\n",
    "s_act = torch.rand(1, 3072)\n",
    "\n",
    "# non-intervened outputs\n",
    "out_b        = b_act @ W_out\n",
    "\n",
    "# different intervention results\n",
    "out_das      = do_das(b_act, s_act, das_mlp8)      @ W_out\n",
    "out_das_null = do_das(b_act, s_act, das_mlp8_null) @ W_out\n",
    "out_das_row  = do_das(b_act, s_act, das_mlp8_row)  @ W_out\n",
    "\n",
    "# different intervention results with unit vectors\n",
    "out_das_null_unit = do_das(b_act, s_act, das_null_unit) @ W_out\n",
    "out_das_row_unit  = do_das(b_act, s_act, das_row_unit)  @ W_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c66c1a",
   "metadata": {},
   "source": [
    "### nullspace effect ~= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "439873d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5157e-06, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_das_null - out_b).sum() # notice the sum here for aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9feac",
   "metadata": {},
   "source": [
    "### diff(v, v_rowspace) = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "120e59d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0345, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_das_row - out_das).sum() # \"illusion\" effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d19eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0538,  0.3925, -0.0303, -0.2352, -0.1694], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out_das_row[0,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c0e919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0550,  0.3923, -0.0299, -0.2363, -0.1697], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out_das[0,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab98a78",
   "metadata": {},
   "source": [
    "### the missing effect: `(s @ v_null.T - b @ v_null.T)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd35bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0993]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s_act - b_act) @ das_mlp8_null.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302eb74",
   "metadata": {},
   "source": [
    "### adding ^ effect back removes the illusion completely\n",
    "caveats: \n",
    "\n",
    "- `das_mlp8_row @ das_mlp8_null.T` is zero, but cannot commute in eqn below\n",
    "\n",
    "- think of `(s_act - b_act) @ das_mlp8_null.T` as a list of scalar values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880b7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_effect = ((s_act - b_act) @ das_mlp8_null.T) @ das_mlp8_row @ W_out\n",
    "out_das_row_composites = out_das_row + missing_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c50bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.0334e-06, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_das_row_composites - out_das).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0898ba",
   "metadata": {},
   "source": [
    "### when ^ is zero then?\n",
    "trivally, when `b` and `s` have the same activations, i.e. `s_act - b_act = 0`.\n",
    "\n",
    "but, there are **two more cases**:\n",
    "\n",
    "1) when `das_mlp8_null` is a zero vector\n",
    "\n",
    "2) when `s_act - b_act` is orthogonal to `das_mlp8_null`; or `any_act` is orthogonal to `das_mlp8_null`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25ca4e",
   "metadata": {},
   "source": [
    "### paradox arises\n",
    "\n",
    "2) above is rare, so calling `das_mlp8_null` to be zero can remove the \"illusion\"; DAS's fault on not being zero.\n",
    "\n",
    "but why DAS can learn non-zero `das_mlp8_null`? it is because 2) above is rare. \n",
    "\n",
    "**thought experiment**: let's train two das directions in parallel `das_mlp8_row` and `das_mlp8_null`. the latter part gets training signals when 2) is not happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10218f3f",
   "metadata": {},
   "source": [
    "### tentative takeaways\n",
    "\n",
    "\"illusion\" arises since models can induce activations in the nullspace (a.k.a. activating dormant path). when dormant path activates, DAS maintains causal efficacy in a data-driven fashion.\n",
    "\n",
    "put in another way, **illusion is more appropriate if you think activation in previous layer has to be in the nullspace of current layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50053c",
   "metadata": {},
   "source": [
    "### the toy example\n",
    "\n",
    "toy example shows similar nullspace effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec36211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_out = torch.tensor([[0], [-2], [1]]).float()\n",
    "das_mlp8 = torch.tensor([[1./torch.sqrt(torch.tensor(2)), -1./torch.sqrt(torch.tensor(2)), 0]])\n",
    "\n",
    "# copy from the illusion code\n",
    "Q, _ = torch.linalg.qr(W_out)\n",
    "das_mlp8_row = das_mlp8 @ Q @ Q.T\n",
    "das_mlp8_null = das_mlp8 - das_mlp8_row\n",
    "das_row_unit = das_mlp8_row / das_mlp8_row.norm()\n",
    "das_null_unit = das_mlp8_null / das_mlp8_null.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e8b1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two random activations\n",
    "b_act = torch.tensor([[1, 0, 1]]).float()\n",
    "s_act = torch.tensor([[2, 0, 2]]).float()\n",
    "\n",
    "# non-intervened outputs\n",
    "out_b        = b_act @ W_out\n",
    "\n",
    "# different intervention results\n",
    "out_das      = do_das(b_act, s_act, das_mlp8)      @ W_out\n",
    "out_das_null = do_das(b_act, s_act, das_mlp8_null) @ W_out\n",
    "out_das_row  = do_das(b_act, s_act, das_mlp8_row)  @ W_out\n",
    "\n",
    "# different intervention results with unit vectors\n",
    "out_das_null_unit = do_das(b_act, s_act, das_null_unit) @ W_out\n",
    "out_das_row_unit  = do_das(b_act, s_act, das_row_unit)  @ W_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df2f2669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_b - out_das_null # null effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8432d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_das - out_das_row # diff(v, v_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1861b12",
   "metadata": {},
   "source": [
    "### what else about the toy example tho?\n",
    "\n",
    "it is a small scale testbed for the nullspace artifact, but anything else?\n",
    "\n",
    "we think it tells us that, if we want to align `H_*` as `x`, we can either do it 1) `H_1` in the rotated basis, or 2) `H_3` in the original basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb38d4",
   "metadata": {},
   "source": [
    "### is this a problem?\n",
    "\n",
    "come back to: what is causal abstraction? \n",
    "\n",
    "**under a set of known interventions, if we cannot distinguish two objections, we claim one is a causal abstraction of the other.**\n",
    "\n",
    "thus, causal abstraction may not provide structural equivalence. there could be parallel circuits, hydra-effect, self-repair, etc.. in other words, having multiple options of abstractions is acceptable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
